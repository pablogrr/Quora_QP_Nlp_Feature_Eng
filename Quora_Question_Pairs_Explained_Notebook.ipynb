{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:16:55.198893Z",
     "start_time": "2021-03-15T16:16:55.196839Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "from nltk import *\n",
    "import os\n",
    "import statistics\n",
    "import editdistance\n",
    "import itertools  \n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:16:56.110580Z",
     "start_time": "2021-03-15T16:16:56.108671Z"
    }
   },
   "outputs": [],
   "source": [
    "cl = sklearn.linear_model.Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv('quora_question_pairs/quora_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:55.642705Z",
     "start_time": "2021-03-15T16:20:55.639644Z"
    }
   },
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    #assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    mylist_of_strings = []\n",
    "    for x in mylist:\n",
    "        mylist_of_strings.append(str(x))\n",
    "\n",
    "    return mylist_of_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T13:06:38.906060Z",
     "start_time": "2020-02-25T13:06:38.852578Z"
    }
   },
   "source": [
    "\n",
    "Use all the questions in train and test partitions to build a single list `all_questions` to fit the `count_vectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation, substituting and lowering\n",
    "Stolen from https://www.kaggle.com/benjaminkz/quora-question-pairs-data-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_transformation_remove_punctuation(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"when's\", \"when is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)  # 除了上面的特殊情况外，“\\'s”只能表示所有格，应替换成“ ”\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text)\n",
    "    text = re.sub(r\" ds \", \" data science \", text)\n",
    "    text = re.sub(r\" ee \", \" electronic engineering \", text)\n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iphone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the us\", \"america\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    \n",
    "    punctuation=\"?:!.,;\"\n",
    "\n",
    "    text = \"\".join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = full_train_df['question1'][0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the step by step guide to invest in share market in india '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = common_words_transformation_remove_punctuation(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    stops     = set(stopwords.words(\"english\"))\n",
    "    token_doc = set(word_tokenize(doc))\n",
    "    token_doc = list(token_doc - stops)\n",
    "    return \" \".join(token_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the step by step guide to invest in share market in india \n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "doc = remove_stopwords(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter    = PorterStemmer()\n",
    "#lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india invest share step guide market\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "doc = stem(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india invest share step guid market \n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "doc = stem(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, save=False):\n",
    "    df = df.copy()\n",
    "    df['question1'] = cast_list_as_strings(list(df[\"question1\"]))\n",
    "    df['question2'] = cast_list_as_strings(list(df[\"question2\"]))\n",
    "        \n",
    "    df['question1_cleaned'] = [common_words_transformation_remove_punctuation(quest) for quest in df[\"question1\"]]\n",
    "    df['question2_cleaned'] = [common_words_transformation_remove_punctuation(quest) for quest in df[\"question2\"]]\n",
    "        \n",
    "    df['question1_no_stops'] = [remove_stopwords(quest) for quest in df[\"question1_cleaned\"]]\n",
    "    df['question2_no_stops'] = [remove_stopwords(quest) for quest in df[\"question2_cleaned\"]]\n",
    "    \n",
    "    df['question1_Porter_stemmed'] = [stem(quest) for quest in df[\"question1_cleaned\"]]\n",
    "    df['question2_Porter_stemmed'] = [stem(quest) for quest in df[\"question2_cleaned\"]]\n",
    "        \n",
    "    df['question1_Lemmatization']  = [lemm(quest) for quest in df[\"question1_cleaned\"]]\n",
    "    df['question2_Lemmatization']  = [lemm(quest) for quest in df[\"question2_cleaned\"]]\n",
    "    \n",
    "    if save==True:\n",
    "        df.to_csv('preprocessed_Quora_full_train_data.csv')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc = preprocess(full_train_df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question1_no_stops</th>\n",
       "      <th>question2_no_stops</th>\n",
       "      <th>question1_Porter_stemmed</th>\n",
       "      <th>question2_Porter_stemmed</th>\n",
       "      <th>question1_Lemmatization</th>\n",
       "      <th>question2_Lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>india invest share step guide market</td>\n",
       "      <td>invest share step guide market</td>\n",
       "      <td>what is the step by step guid to invest in sha...</td>\n",
       "      <td>what is the step by step guid to invest in sha...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the story of kohinoor koh i noor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>koh noor diamond kohinoor story</td>\n",
       "      <td>stole government indian happen koh back noor d...</td>\n",
       "      <td>what is the stori of kohinoor koh i noor diamond</td>\n",
       "      <td>what would happen if the indian govern stole t...</td>\n",
       "      <td>what is the story of kohinoor koh i noor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>speed internet using increase vpn connection</td>\n",
       "      <td>hacking speed increased internet dns</td>\n",
       "      <td>how can i increas the speed of my internet con...</td>\n",
       "      <td>how can internet speed be increas by hack thro...</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math 23  24  math is d...</td>\n",
       "      <td>lonely solve mentally</td>\n",
       "      <td>24 remainder 23 divided find math</td>\n",
       "      <td>whi am i mental veri lone how can i solv it</td>\n",
       "      <td>find the remaind when math 23 24 math is divid...</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math 23 24 math is div...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>which one dissolve in water quickly sugar  sal...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>sugar methane quickly water one salt carbon ox...</td>\n",
       "      <td>fish water salt survive would</td>\n",
       "      <td>which one dissolv in water quickli sugar salt ...</td>\n",
       "      <td>which fish would surviv in salt water</td>\n",
       "      <td>which one dissolve in water quickly sugar salt...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what is the story of kohinoor koh i noor diamond    \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3  why am i mentally very lonely how can i solve it    \n",
       "4  which one dissolve in water quickly sugar  sal...   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what would happen if the indian government sto...   \n",
       "2  how can internet speed be increased by hacking...   \n",
       "3  find the remainder when math 23  24  math is d...   \n",
       "4            which fish would survive in salt water    \n",
       "\n",
       "                                  question1_no_stops  \\\n",
       "0               india invest share step guide market   \n",
       "1                    koh noor diamond kohinoor story   \n",
       "2       speed internet using increase vpn connection   \n",
       "3                              lonely solve mentally   \n",
       "4  sugar methane quickly water one salt carbon ox...   \n",
       "\n",
       "                                  question2_no_stops  \\\n",
       "0                     invest share step guide market   \n",
       "1  stole government indian happen koh back noor d...   \n",
       "2               hacking speed increased internet dns   \n",
       "3                  24 remainder 23 divided find math   \n",
       "4                      fish water salt survive would   \n",
       "\n",
       "                            question1_Porter_stemmed  \\\n",
       "0  what is the step by step guid to invest in sha...   \n",
       "1  what is the stori of kohinoor koh i noor diamond    \n",
       "2  how can i increas the speed of my internet con...   \n",
       "3       whi am i mental veri lone how can i solv it    \n",
       "4  which one dissolv in water quickli sugar salt ...   \n",
       "\n",
       "                            question2_Porter_stemmed  \\\n",
       "0  what is the step by step guid to invest in sha...   \n",
       "1  what would happen if the indian govern stole t...   \n",
       "2  how can internet speed be increas by hack thro...   \n",
       "3  find the remaind when math 23 24 math is divid...   \n",
       "4             which fish would surviv in salt water    \n",
       "\n",
       "                             question1_Lemmatization  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what is the story of kohinoor koh i noor diamond    \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3  why am i mentally very lonely how can i solve it    \n",
       "4  which one dissolve in water quickly sugar salt...   \n",
       "\n",
       "                             question2_Lemmatization  \n",
       "0  what is the step by step guide to invest in sh...  \n",
       "1  what would happen if the indian government sto...  \n",
       "2  how can internet speed be increased by hacking...  \n",
       "3  find the remainder when math 23 24 math is div...  \n",
       "4            which fish would survive in salt water   "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting empty questions\n",
    "There are some questions that are empty. We drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_len(question):\n",
    "    num_words = len(question.split())\n",
    "    return num_words\n",
    "\n",
    "#Dropping all the sample where there is not one of the two questions\n",
    "len_1   = full_train_proc.question1_cleaned.apply(lambda x: cont_len(x))\n",
    "empty_1 = len_1[len_1==0].index\n",
    "\n",
    "len_2   = full_train_proc.question2_cleaned.apply(lambda x: cont_len(x))\n",
    "empty_2 = len_2[len_2==0].index\n",
    "\n",
    "to_delete = (empty_2).union(set(empty_1))\n",
    "\n",
    "full_train_proc = full_train_proc.drop(index=to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Other Features Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These functions generate new feature in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jsreu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_common(q1, q2):\n",
    "    \"\"\"\n",
    "    counts number of words in common\n",
    "    \"\"\"\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1.intersection(q2))\n",
    "\n",
    "def len_not_common(q1,q2):\n",
    "    \"\"\"\n",
    "    counts number of words not in common\n",
    "    \"\"\"\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1 ^ q2)\n",
    "\n",
    "#Adding mean distance between common words \n",
    "def mean_dist_not_com(q1,q2):\n",
    "    \"\"\"\n",
    "    mean edit distance between words not in common\n",
    "    \"\"\"\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    not_comm1 = (q1 ^ q2) - q1\n",
    "    if len(not_comm1)==0 : not_comm1={''}\n",
    "    not_comm2 = (q1 ^ q2) - q2\n",
    "    if len(not_comm2)==0 : not_comm2={''}\n",
    "    return statistics.mean([editdistance.eval(i[0],i[1]) for i in itertools.product(not_comm1, not_comm2)])\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString)) \n",
    "def both_number(q1,q2):\n",
    "    \"\"\"\n",
    "    If in both questions ther is a number\n",
    "    \"\"\"\n",
    "    return hasNumbers(q1) *  hasNumbers(q2) \n",
    "\n",
    "def average_len(question):\n",
    "    \"\"\"\n",
    "    get average number of words\n",
    "    \"\"\"\n",
    "    num_words = len(question.split())\n",
    "    return len(question)/num_words   \n",
    "\n",
    "def levenshtein(q1, q2): \n",
    "    \"\"\"\n",
    "    levenshtein distance (for strings of unequal length)\n",
    "    \"\"\"\n",
    "    #create initial array (two for loops since q1 and q2 can differ in length)\n",
    "    dist_array = []\n",
    "    for i in range(len(q1)+1):\n",
    "        dist_array.append([0]*(len(q2)+1))\n",
    "        dist_array[i][0] = i\n",
    "    for j in range(len(q2)+1):\n",
    "        dist_array[0][j] = j\n",
    "\n",
    "    dist = [0]*3\n",
    "    for i in range(1,len(q1)+1):\n",
    "        for j in range(1,len(q2)+1):\n",
    "            dist[0] = dist_array[i-1][j-1] if q1[i-1]==q2[j-1] else dist_array[i-1][j-1]+1\n",
    "            dist[1] = dist_array[i][j-1]+1\n",
    "            dist[2] = dist_array[i-1][j]+1\n",
    "            dist_array[i][j]=min(dist)\n",
    "    \n",
    "    return dist_array[i][j]\n",
    "\n",
    "#from math import*\n",
    "def jaccard_similarity(row):\n",
    "    q1 = row[\"question1\"]\n",
    "    q2 = row[\"question2\"]\n",
    "    set1 = set(q1.split()) \n",
    "    set2 = set(q2.split())\n",
    "    same = set1.intersection(set2)\n",
    "    return float(len(same)) / (len(set1) + len(set2) - len(same))\n",
    "\n",
    "def common_tokens(string_1,string_2):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    x = nltk.word_tokenize(string_1)\n",
    "    y = nltk.word_tokenize(string_2)   \n",
    " \n",
    "    common_tokens = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common_tokens)\n",
    "\n",
    "def common_count(string_1,string_2, word_type = \"NOUN\"):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    tagged_1 = nltk.pos_tag(nltk.word_tokenize(string_1),tagset='universal')\n",
    "    tagged_2 = nltk.pos_tag(nltk.word_tokenize(string_2),tagset='universal')\n",
    "    \n",
    "    x = list([])\n",
    "    for word in tagged_1:\n",
    "        if word[1] == word_type:\n",
    "            x.append(word[0])\n",
    "                     \n",
    "    y = list([])\n",
    "    for word in tagged_2:\n",
    "        if word[1] == word_type:\n",
    "            y.append(word[0])\n",
    " \n",
    "    common = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common)\n",
    "\n",
    "def length_diff_characters(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in character length of two questions\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    y = str(y)\n",
    "    return abs(len(y)-len(x))\n",
    "\n",
    "def length_diff_tokens(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in number of tokens in two questions\n",
    "    \"\"\"\n",
    "    return abs(len(nltk.word_tokenize(y))-len(nltk.word_tokenize(x)))\n",
    "\n",
    "def common_numbers(x,y):\n",
    "    \"\"\"\n",
    "    count numbers present in both tokens\n",
    "    \"\"\"\n",
    "    x_numbers = re.findall(r'\\b\\d+\\b', x)\n",
    "    y_numbers = re.findall(r'\\b\\d+\\b', y)\n",
    "    \n",
    "    common_numbers = len(list(set(x_numbers).intersection(y_numbers)))\n",
    "    return(common_numbers)\n",
    "\n",
    "def first_word(x,y):\n",
    "    \"\"\"\n",
    "    boolean if the first word is the same\n",
    "    \"\"\"\n",
    "    word_list_x = x.split()  # list of words\n",
    "    word_list_y = y.split()  # list of words\n",
    "    return word_list_x[0] == word_list_y[0]\n",
    "\n",
    "def last_word(x,y):\n",
    "    \"\"\"\n",
    "    boolean if the last word is the same\n",
    "    \"\"\"\n",
    "    word_list_x = x.split()  # list of words\n",
    "    word_list_y = y.split()  # list of words\n",
    "    return word_list_x[-1] == word_list_y[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add_features() apply to the data all the feature generator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_features(df, scaler, col1, col2, save=False):\n",
    "    df = df.copy()\n",
    "    col1_to_transform = list(full_train_proc.columns).index(col1)\n",
    "    col2_to_transform = list(full_train_proc.columns).index(col2)\n",
    "\n",
    "    df['len_q1'] = [len(s) for s in df[col1]] \n",
    "    df['len_q2'] = [len(s) for s in df[col2]]\n",
    "    df['len_q1'] = scaler.fit_transform(df[['len_q1']])\n",
    "    df['len_q2'] = scaler.fit_transform(df[['len_q2']])\n",
    "    \n",
    "    df['len_common'] = [len_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_common'] = scaler.fit_transform(df[['len_common']])\n",
    "\n",
    "    df['len_not_common'] = [len_not_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_not_common'] = scaler.fit_transform(df[['len_not_common']])\n",
    "    \n",
    "    df['mean_dist_not_com'] = [mean_dist_not_com(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['mean_dist_not_com'] = scaler.fit_transform(df[['mean_dist_not_com']])\n",
    "    \n",
    "    df['both_number'] = [both_number(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    \n",
    "    df[\"avg_len_q1\"]= df[col1].apply(lambda x: average_len(x))\n",
    "    df[\"avg_len_q2\"]= df[col2].apply(lambda x: average_len(x))\n",
    "    df['avg_len_q1'] = scaler.fit_transform(df[['avg_len_q1']])\n",
    "    df['avg_len_q2'] = scaler.fit_transform(df[['avg_len_q2']])\n",
    "\n",
    "    df['edit_distance'] = [editdistance.eval(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['edit_distance'] = scaler.fit_transform(df[['edit_distance']])\n",
    "    \n",
    "    df['levenshtein_distance'] = [levenshtein(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['levenshtein_distance'] = scaler.fit_transform(df[['levenshtein_distance']])\n",
    "    \n",
    "    df[\"common_nouns\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"NOUN\"), axis=1)\n",
    "    df['common_nouns'] = scaler.fit_transform(df[['common_nouns']])\n",
    "\n",
    "    df[\"common_verbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"VERB\"), axis=1)\n",
    "    df['common_verbs'] = scaler.fit_transform(df[['common_verbs']])\n",
    "    \n",
    "    df[\"common_adjectives\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADJ\"), axis=1)\n",
    "    df['common_adjectives'] = scaler.fit_transform(df[['common_adjectives']])\n",
    "    \n",
    "    df[\"common_adverbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADV\"), axis=1)\n",
    "    df['common_adverbs'] = scaler.fit_transform(df[['common_adverbs']])\n",
    "\n",
    "    df[\"common_tokens\"] = df.apply(lambda df: common_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['common_tokens'] = scaler.fit_transform(df[['common_tokens']])\n",
    "\n",
    "    df[\"length_diff_characters\"] = df.apply(lambda df: length_diff_characters(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_characters'] = scaler.fit_transform(df[['length_diff_characters']])\n",
    "\n",
    "    df[\"length_diff_tokens\"] = df.apply(lambda df: length_diff_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_tokens'] = scaler.fit_transform(df[['length_diff_tokens']])\n",
    "\n",
    "    df[\"common_numbers\"] = df.apply(lambda df: length_diff_characters(df.question1, df.question2), axis=1)\n",
    "    df['common_numbers'] = scaler.fit_transform(df[['common_numbers']])\n",
    "\n",
    "    df['first_word'] = df.apply(lambda df: first_word(df[col1], df[col2]), axis=1)*1\n",
    "    df['last_word'] = df.apply(lambda df: last_word(df[col1], df[col2]), axis=1)*1\n",
    "    \n",
    "    df[\"jaccard_similarity\"] = df.apply(lambda x: jaccard_similarity(x), axis=1)\n",
    "    df['jaccard_similarity'] = scaler.fit_transform(df[['jaccard_similarity']])\n",
    "    \n",
    "    if save==True:\n",
    "        df.to_csv('features_added_Quora_full_train_data.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "full_train_proc_plus_feat = Add_features(full_train_proc, scaler, 'question1_cleaned', 'question2_cleaned', save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Two Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_on_q1_q2(df, model):\n",
    "    q_list1 = list(df[\"question1_cleaned\"])\n",
    "    q_list2 = list(df[\"question2_cleaned\"])\n",
    "    all_questions = q_list1 + q_list2 \n",
    "    model.fit(all_questions)\n",
    "    return\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    ############### Begin exercise ###################\n",
    "    # what is kaggle                  q1\n",
    "    # What is the kaggle platform     q2\n",
    "    X_q1 = count_vectorizer.transform(q1_casted)\n",
    "    X_q2 = count_vectorizer.transform(q2_casted)    \n",
    "    X_q1q2 = scipy.sparse.hstack((X_q1,X_q2))\n",
    "    ############### End exercise ###################\n",
    "\n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    #svm_model = svm.SVC()\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    #svm_model.fit(X_tr_q1q2, y_train)\n",
    "                                                   \n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_val, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_test, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "        \n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "                                              \n",
    "                                                       \n",
    "                                                       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8560015673032263, logistic_val_acc:0.7953934745032775, logistic_test_acc:0.7956985843493918\n",
      "xgb_train_acc:0.8659191549450259, xgb_val_acc:0.8093737107889942, xgb_test_acc:0.8097783288425393\n"
     ]
    }
   ],
   "source": [
    "logistic_simple, xgb_model_simple, acc_logistic_simple, acc_xgboost_simple = \\\n",
    "train_models(full_train_proc_plus_feat, get_features_from_df, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:59:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8416865189699401, logistic_val_acc:0.7958238445455491, logistic_test_acc:0.797239576925177\n",
      "xgb_train_acc:0.9070173792036841, xgb_val_acc:0.8234103363606295, xgb_test_acc:0.827242512987826\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. More advanced models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Adding Lemmatization/Stemming and removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc_plus_feat = full_train_proc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_lemma(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_Lemmatization\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2_Lemmatization\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsreu\\anaconda3\\envs\\NLP\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:25:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:26:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.855750814661588, logistic_val_acc:0.8068484115751718, logistic_test_acc:0.8085147242608809\n",
      "xgb_train_acc:0.9647508218153511, xgb_val_acc:0.8711220416076965, xgb_test_acc:0.8764785359835098\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf_lemma, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc_plus_feat = full_train_proc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_stem(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_Porter_stemmed\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2_Porter_stemmed\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsreu\\anaconda3\\envs\\NLP\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:46:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8198517225646291, logistic_val_acc:0.7864257782597975, logistic_test_acc:0.7861417455061237\n",
      "xgb_train_acc:0.961030138328266, xgb_val_acc:0.8669393473334588, xgb_test_acc:0.8700238109795451\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf_stem, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc_plus_feat = full_train_proc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_stops(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_no_stops\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2_no_stops\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsreu\\anaconda3\\envs\\NLP\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:12:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:12:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8597280270805987, logistic_val_acc:0.7999421615386503, logistic_test_acc:0.8012380254227004\n",
      "xgb_train_acc:0.881222092059111, xgb_val_acc:0.8283344708581338, xgb_test_acc:0.8343091186477951\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf_stops, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Adding additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=True):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:56:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9074969597126898, logistic_val_acc:0.8726778599886169, logistic_test_acc:0.8772466341269656\n",
      "xgb_train_acc:0.9766716766767485, xgb_val_acc:0.8987541590750521, xgb_test_acc:0.9019259149390173\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf_sim, xgb_model_simple_tf_sim, acc_logistic_tf_sim, acc_xgboost_tf_sim = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train_val_test(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    return  train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_a_column_feat(data, col, sparse_matrix):\n",
    "    feat_q = data[col].to_numpy().reshape(len(data[col]),1)\n",
    "        \n",
    "    return scipy.sparse.hstack((sparse_matrix,feat_q)).tocsr()        \n",
    "\n",
    "def train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, col_list):    \n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    \n",
    "    for col in col_list:\n",
    "        X_train_q1q2      = add_a_column_feat(train_df, col, X_train_q1q2)\n",
    "        X_val_q1q2        = add_a_column_feat(val_df, col, X_val_q1q2)\n",
    "        X_test_q1q2       = add_a_column_feat(test_df, col, X_test_q1q2)\n",
    "    print('...features added...')\n",
    "\n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    print('...logistic model fitted...')\n",
    "\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    print('...model fitted...')\n",
    "\n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_val, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_test, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2 = vectorize_train_val_test(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:10:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9105569629864025, logistic_val_acc:0.877148443480405, logistic_test_acc:0.8810641865271815\n",
      "xgb_train_acc:0.9893273664276654, xgb_val_acc:0.9060839423181429, xgb_test_acc:0.906456667632005\n"
     ]
    }
   ],
   "source": [
    "cols_lenght = ['len_q1', 'len_q2', 'len_common', 'len_not_common', 'avg_len_q1', 'avg_len_q2']\n",
    "logistic_simple_tf_sim_len, xgb_model_simple_tf_sim_len, acc_logistic_tfidf_sim_len, acc_xgboost_tfidf_sim_len \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, cols_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2 = vectorize_train_val_test(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...features added...\n",
      "...logistic model fitted...\n",
      "[22:45:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:45:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "...model fitted...\n",
      "logistic_train_acc:0.9122547681146612, logistic_val_acc:0.879432834809194, logistic_test_acc:0.8829894326066793\n",
      "xgb_train_acc:0.991570516727775, xgb_val_acc:0.9099064872805638, xgb_test_acc:0.9095038783787787\n"
     ]
    }
   ],
   "source": [
    "cols_dist = cols_lenght + ['mean_dist_not_com', 'edit_distance']\n",
    "\n",
    "logistic_simple_tf_sim_len_dist, xgb_model_simple_tf_sim_len_dist, acc_logistic_tfidf_sim_len_dist, acc_xgboost_tfidf_sim_len_dist \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, cols_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...features added...\n",
      "...logistic model fitted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:50:46] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:51:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "...model fitted...\n",
      "logistic_train_acc:0.9160962845200364, logistic_val_acc:0.8843373353169895, logistic_test_acc:0.8881570223833523\n",
      "xgb_train_acc:0.9933322981345474, xgb_val_acc:0.9168239801135734, xgb_test_acc:0.9158050435110939\n"
     ]
    }
   ],
   "source": [
    "all_cols = cols_dist + ['both_number', 'first_word', 'last_word', 'length_diff_characters', 'length_diff_tokens', 'common_numbers']\n",
    "\n",
    "logistic_simple_tf_sim_len_dist_extra, xgb_model_simple_tf_sim_len_dist_extra, acc_logistic_tfidf_sim_len_dist_extra, acc_xgboost_tfidf_sim_len_dist_extra \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, all_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame({'BOW_No_Feat_Logistic':acc_logistic_simple,'BOW_No_Feat_XGB':acc_xgboost_simple,\n",
    "                     'TFIDF_No_Feat_Logistic':acc_logistic_simple_tf,'TFIDF_No_Feat_XGB':acc_xgboost_simple_tf,\n",
    "                     'TFIDF_Cos_Sim_Logistic':acc_logistic_tf_sim,'TFIDF_Cos_Sim_XGB':acc_xgboost_tf_sim,\n",
    "                     'TFIDF_Cos_Sim_Len_Logistic':acc_logistic_tfidf_sim_len,'TFIDF_Cos_Sim_Len_XGB':acc_xgboost_tfidf_sim_len,\n",
    "                     'TFIDF_Cos_Sim_Len_Dist_Logistic':acc_logistic_tfidf_sim_len_dist,'TFIDF_Cos_Sim_Len_Dist_XGB':acc_xgboost_tfidf_sim_len_dist,\n",
    "                     'TFIDF_Cos_Sim_Len_Dist_ExtraFeat_Logistic':acc_logistic_tfidf_sim_len_dist_extra,'TFIDF_Cos_Sim_Len_Dist_ExtraFeat_XGB':acc_xgboost_tfidf_sim_len_dist_extra,\n",
    "                    }, index=['Train','Validation', 'Test'])\n",
    "hist.to_csv('hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW_No_Feat_Logistic</th>\n",
       "      <th>BOW_No_Feat_XGB</th>\n",
       "      <th>TFIDF_No_Feat_Logistic</th>\n",
       "      <th>TFIDF_No_Feat_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_ExtraFeat_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_ExtraFeat_XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.856002</td>\n",
       "      <td>0.865919</td>\n",
       "      <td>0.841687</td>\n",
       "      <td>0.907017</td>\n",
       "      <td>0.907497</td>\n",
       "      <td>0.976672</td>\n",
       "      <td>0.910557</td>\n",
       "      <td>0.989327</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>0.991571</td>\n",
       "      <td>0.916096</td>\n",
       "      <td>0.993332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.795393</td>\n",
       "      <td>0.809374</td>\n",
       "      <td>0.795824</td>\n",
       "      <td>0.823410</td>\n",
       "      <td>0.872678</td>\n",
       "      <td>0.898754</td>\n",
       "      <td>0.877148</td>\n",
       "      <td>0.906084</td>\n",
       "      <td>0.879433</td>\n",
       "      <td>0.909906</td>\n",
       "      <td>0.884337</td>\n",
       "      <td>0.916824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.809778</td>\n",
       "      <td>0.797240</td>\n",
       "      <td>0.827243</td>\n",
       "      <td>0.877247</td>\n",
       "      <td>0.901926</td>\n",
       "      <td>0.881064</td>\n",
       "      <td>0.906457</td>\n",
       "      <td>0.882989</td>\n",
       "      <td>0.909504</td>\n",
       "      <td>0.888157</td>\n",
       "      <td>0.915805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BOW_No_Feat_Logistic  BOW_No_Feat_XGB  TFIDF_No_Feat_Logistic  \\\n",
       "Train                   0.856002         0.865919                0.841687   \n",
       "Validation              0.795393         0.809374                0.795824   \n",
       "Test                    0.795699         0.809778                0.797240   \n",
       "\n",
       "            TFIDF_No_Feat_XGB  TFIDF_Cos_Sim_Logistic  TFIDF_Cos_Sim_XGB  \\\n",
       "Train                0.907017                0.907497           0.976672   \n",
       "Validation           0.823410                0.872678           0.898754   \n",
       "Test                 0.827243                0.877247           0.901926   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Logistic  TFIDF_Cos_Sim_Len_XGB  \\\n",
       "Train                         0.910557               0.989327   \n",
       "Validation                    0.877148               0.906084   \n",
       "Test                          0.881064               0.906457   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Dist_Logistic  TFIDF_Cos_Sim_Len_Dist_XGB  \\\n",
       "Train                              0.912255                    0.991571   \n",
       "Validation                         0.879433                    0.909906   \n",
       "Test                               0.882989                    0.909504   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Dist_ExtraFeat_Logistic  \\\n",
       "Train                                        0.916096   \n",
       "Validation                                   0.884337   \n",
       "Test                                         0.888157   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Dist_ExtraFeat_XGB  \n",
       "Train                                   0.993332  \n",
       "Validation                              0.916824  \n",
       "Test                                    0.915805  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end adding features we always improve: the biggest improvement after adding cosine similarity. The final best score in validation is 0.915805 with a tes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
