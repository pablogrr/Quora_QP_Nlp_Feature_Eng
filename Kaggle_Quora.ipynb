{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "from nltk import *\n",
    "import os\n",
    "import statistics\n",
    "import editdistance\n",
    "import itertools  \n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv('quora_question_pairs/quora_train.csv')\n",
    "#train_df, test_df = sklearn.model_selection.train_test_split(full_train_df, test_size=0.1,random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all in string\n",
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    #assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    mylist_of_strings = []\n",
    "    for x in mylist:\n",
    "        mylist_of_strings.append(str(x))\n",
    "\n",
    "    return mylist_of_strings\n",
    "\n",
    "#Removing Stops Words\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    stops     = set(stopwords.words(\"english\"))\n",
    "    token_doc = set(word_tokenize(doc))\n",
    "    token_doc = list(token_doc - stops)\n",
    "    return \" \".join(token_doc)\n",
    "\n",
    "#Adding len Words in common feat\n",
    "def len_common(q1, q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1.intersection(q2))\n",
    "\n",
    "#Adding len common words in common feat\n",
    "def len_not_common(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1 ^ q2)\n",
    "\n",
    "#Adding mean distance between common words \n",
    "def mean_dist_not_com(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    not_comm1 = (q1 ^ q2) - q1\n",
    "    if len(not_comm1)==0 : not_comm1={''}\n",
    "    not_comm2 = (q1 ^ q2) - q2\n",
    "    if len(not_comm2)==0 : not_comm2={''}\n",
    "    return statistics.mean([editdistance.eval(i[0],i[1]) for i in itertools.product(not_comm1, not_comm2)])\n",
    "\n",
    "def hasNumbers(inputString): return bool(re.search(r'\\d', inputString)) \n",
    "def both_number(q1,q2): return hasNumbers(q1) *  hasNumbers(q2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, scaler):\n",
    "    df = df.copy()\n",
    "    df['question1_string'] = cast_list_as_strings(list(df[\"question1\"]))\n",
    "    df['question2_string'] = cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    df['question1_no_stops'] = [remove_stopwords(quest) for quest in df[\"question1_string\"]]\n",
    "    df['question2_no_stops'] = [remove_stopwords(quest) for quest in df[\"question2_string\"]]\n",
    "    \n",
    "    df['len_q1'] = [len(s) for s in df['question1_string']] \n",
    "    df['len_q2'] = [len(s) for s in df['question2_string']]\n",
    "    df['len_q1'] = scaler.fit_transform(df[['len_q1']])\n",
    "    df['len_q2'] = scaler.fit_transform(df[['len_q2']])\n",
    "    \n",
    "    df['len_common'] = [len_common(df.loc[i,'question1_string'],df.loc[i,'question2_string']) for i in range(len(df))]\n",
    "    df['len_common'] = scaler.fit_transform(df[['len_common']])\n",
    "\n",
    "    df['len_not_common'] = [len_not_common(df.loc[i,'question1_string'],df.loc[i,'question2_string']) for i in range(len(df))]\n",
    "    df['len_not_common'] = scaler.fit_transform(df[['len_not_common']])\n",
    "    \n",
    "    df['mean_dist_not_com'] = [mean_dist_not_com(df.loc[i,'question1_string'],df.loc[i,'question2_string']) for i in range(len(df))]\n",
    "    df['mean_dist_not_com'] = scaler.fit_transform(df[['mean_dist_not_com']])\n",
    "    \n",
    "    df['both_number'] = [both_number(df.loc[i,'question1_string'],df.loc[i,'question2_string']) for i in range(len(df))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('data_preprocessed.csv')\n",
    "df_full = df_full.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_on_q1_q2(df, model):\n",
    "    q_list1 = list(df[\"question1_string\"])\n",
    "    q_list2 = list(df[\"question2_string\"])\n",
    "    all_questions = q_list1 + q_list2 \n",
    "    model.fit(all_questions)\n",
    "    return\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    ############### Begin exercise ###################\n",
    "    # what is kaggle                  q1\n",
    "    # What is the kaggle platform     q2\n",
    "    X_q1 = count_vectorizer.transform(q1_casted)\n",
    "    X_q2 = count_vectorizer.transform(q2_casted)    \n",
    "    X_q1q2 = scipy.sparse.hstack((X_q1,X_q2))\n",
    "    ############### End exercise ###################\n",
    "\n",
    "    return X_q1q2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    #svm_model = svm.SVC()\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    #svm_model.fit(X_tr_q1q2, y_train)\n",
    "                                                   \n",
    "    logistic_train_acc = logistic.score(X_train_q1q2, y_train)                                                       \n",
    "    logistic_val_acc   = logistic.score(X_val_q1q2, y_val)\n",
    "    logistic_test_acc  = logistic.score(X_test_q1q2, y_test)\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = f1_score(y_train, xgb_model.predict(X_train_q1q2), average='macro')\n",
    "    xgb_val_acc        = f1_score(y_val, xgb_model.predict(X_val_q1q2), average='macro')\n",
    "    xgb_test_acc       = f1_score(y_test, xgb_model.predict(X_test_q1q2), average='macro')\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "                                                       \n",
    "                                                       \n",
    "                                                       \n",
    "                           \n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "fit_on_q1_q2(df_full, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:57:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:57:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.809157983018237, logistic_val_acc:0.754894813580504, logistic_test_acc:0.7585951026465496\n",
      "xgb_train_acc:0.8219160543002528, xgb_val_acc:0.7624781972290291, xgb_test_acc:0.7669158422548783\n"
     ]
    }
   ],
   "source": [
    "acc_logistic, acc_xgboost = train_models(df_full, get_features_from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(df_full, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:32:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:32:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.79214059014093, logistic_val_acc:0.7641116434076234, logistic_test_acc:0.765075439030423\n",
      "xgb_train_acc:0.8791637191229951, xgb_val_acc:0.7719671763079887, xgb_test_acc:0.7753559945880965\n"
     ]
    }
   ],
   "source": [
    "acc_logistic_tfidf, acc_xgboost_tfidf = train_models(df_full, get_tfidf, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=True):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:17:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:18:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8256052973452976, logistic_val_acc:0.7962403665902937, logistic_test_acc:0.7995547860499629\n",
      "xgb_train_acc:0.9179797452089911, xgb_val_acc:0.8042715469580926, xgb_test_acc:0.8025583271975577\n"
     ]
    }
   ],
   "source": [
    "acc_logistic_tfidf_sim, acc_xgboost_tfidf_sim = train_models(df_full, get_tfidf, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features: len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_a_column_feat_single(df, col_list, sparse_matrix):\n",
    "\n",
    "    for col_q in col_list:\n",
    "        feat_q = df[col_q].to_numpy().reshape(len(df[col_q]),1)\n",
    "        sparse_matrix = scipy.sparse.hstack((sparse_matrix,feat_q)).tocsr()\n",
    "    \n",
    "    return sparse_matrix     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, get_feat_model, vectorizer_func, col_list):\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    #svm_model = svm.SVC()\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    X_train_q1q2      = add_a_column_feat_single(train_df, col_list, X_train_q1q2)\n",
    "    X_val_q1q2        = add_a_column_feat_single(val_df, col_list, X_val_q1q2)\n",
    "    X_test_q1q2       = add_a_column_feat_single(test_df, col_list, X_test_q1q2)\n",
    "    \n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    #svm_model.fit(X_tr_q1q2, y_train)\n",
    "                                                   \n",
    "    logistic_train_acc = logistic.score(X_train_q1q2, y_train)                                                       \n",
    "    logistic_val_acc   = logistic.score(X_val_q1q2, y_val)\n",
    "    logistic_test_acc  = logistic.score(X_test_q1q2, y_test)\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = f1_score(y_train, xgb_model.predict(X_train_q1q2), average='macro')\n",
    "    xgb_val_acc        = f1_score(y_val, xgb_model.predict(X_val_q1q2), average='macro')\n",
    "    xgb_test_acc       = f1_score(y_test, xgb_model.predict(X_test_q1q2), average='macro')\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_logistic_tfidf_sim_len, acc_xgboost_tfidf_sim_len = train_models(df_full, get_tfidf, tfidf, ['len_q1', 'len_q2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features: mean_dist_not_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_logistic_tfidf_sim_len, acc_xgboost_tfidf_sim_len = train_models(df_full, get_tfidf, tfidf, ['len_q1', 'len_q2', 'mean_dist_not_com'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
