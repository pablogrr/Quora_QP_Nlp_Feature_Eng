{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing data & cast_list_as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:16:55.198893Z",
     "start_time": "2021-03-15T16:16:55.196839Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "from nltk import *\n",
    "import os\n",
    "import statistics\n",
    "import editdistance\n",
    "import itertools  \n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:16:56.110580Z",
     "start_time": "2021-03-15T16:16:56.108671Z"
    }
   },
   "outputs": [],
   "source": [
    "cl = sklearn.linear_model.Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv('quora_question_pairs/quora_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.loc[3306,'question1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T17:06:26.786165Z",
     "start_time": "2021-03-15T17:06:26.229644Z"
    }
   },
   "outputs": [],
   "source": [
    "path_data =  os.path.expanduser('~') \n",
    "\n",
    "\n",
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(os.path.join(path_data,\n",
    "                                    os.path.join(\"Datasets\", \"kaggle_datasets\", \"quora\", \"quora_train_data.csv\")))\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(os.path.join(path_data,\n",
    "                                    os.path.join(\"Datasets\", \"kaggle_datasets\", \"quora\", \"test.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T12:54:11.684056Z",
     "start_time": "2020-02-25T12:54:11.681299Z"
    }
   },
   "source": [
    "###  Exercise 1:  `cast_list_as_strings`\n",
    "\n",
    "Build a function  **`cast_list_as_strings`** that casts each element in the input list to a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q = list(full_train_df[\"question1\"]) + list(full_train_df[\"question2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:54.544067Z",
     "start_time": "2021-03-15T16:20:54.406412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['float', 'str'], dtype='<U5')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "types_ = [type(q).__name__ for q in all_q]\n",
    "np.unique(types_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:55.642705Z",
     "start_time": "2021-03-15T16:20:55.639644Z"
    }
   },
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    #assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    mylist_of_strings = []\n",
    "    for x in mylist:\n",
    "        mylist_of_strings.append(str(x))\n",
    "\n",
    "    return mylist_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:57.439436Z",
     "start_time": "2021-03-15T16:20:57.263710Z"
    }
   },
   "outputs": [],
   "source": [
    "q1_train =  cast_list_as_strings(list(full_train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(full_train_df[\"question2\"]))\n",
    "#q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "#q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:59.132475Z",
     "start_time": "2021-03-15T16:20:59.024672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['str'], dtype='<U3')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "types_ = [type(q).__name__ for q in q2_train]\n",
    "np.unique(types_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T13:06:38.906060Z",
     "start_time": "2020-02-25T13:06:38.852578Z"
    }
   },
   "source": [
    "\n",
    "Use all the questions in train and test partitions to build a single list `all_questions` to fit the `count_vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:21:01.126503Z",
     "start_time": "2021-03-15T16:21:01.112286Z"
    }
   },
   "outputs": [],
   "source": [
    "all_questions = q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_train_df.copy()\n",
    "df['question1_string'] = cast_list_as_strings(list(df[\"question1\"]))\n",
    "df['question2_string'] = cast_list_as_strings(list(df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation, substituting and lowering\n",
    "Stolen from https://www.kaggle.com/benjaminkz/quora-question-pairs-data-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_transformation_remove_punctuation(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"when's\", \"when is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)  # 除了上面的特殊情况外，“\\'s”只能表示所有格，应替换成“ ”\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text)\n",
    "    text = re.sub(r\" ds \", \" data science \", text)\n",
    "    text = re.sub(r\" ee \", \" electronic engineering \", text)\n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iphone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the us\", \"america\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    \n",
    "    punctuation=\"?:!.,;\"\n",
    "\n",
    "    text = \"\".join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = full_train_df['question1'][0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the step by step guide to invest in share market in india '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = common_words_transformation_remove_punctuation(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    stops     = set(stopwords.words(\"english\"))\n",
    "    token_doc = set(word_tokenize(doc))\n",
    "    token_doc = list(token_doc - stops)\n",
    "    return \" \".join(token_doc)\n",
    "\n",
    "df['question1_no_stops'] = [remove_stopwords(quest) for quest in df[\"question1_string\"]]\n",
    "df['question2_no_stops'] = [remove_stopwords(quest) for quest in df[\"question2_string\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share market invest india guide step\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "doc = remove_stopwords(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter    = PorterStemmer()\n",
    "#lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share market invest india guide step\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "doc = stem(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share market invest india guid step \n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "doc = stem(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, save=False):\n",
    "    df = df.copy()\n",
    "    df['question1'] = cast_list_as_strings(list(df[\"question1\"]))\n",
    "    df['question2'] = cast_list_as_strings(list(df[\"question2\"]))\n",
    "        \n",
    "    df['question1_cleaned'] = [common_words_transformation_remove_punctuation(quest) for quest in df[\"question1\"]]\n",
    "    df['question2_cleaned'] = [common_words_transformation_remove_punctuation(quest) for quest in df[\"question2\"]]\n",
    "        \n",
    "    df['question1_no_stops'] = [remove_stopwords(quest) for quest in df[\"question1_cleaned\"]]\n",
    "    df['question2_no_stops'] = [remove_stopwords(quest) for quest in df[\"question2_cleaned\"]]\n",
    "    \n",
    "    df['question1_Porter_stemmed'] = [stem(quest) for quest in df[\"question1_no_stops\"]]\n",
    "    df['question2_Porter_stemmed'] = [stem(quest) for quest in df[\"question2_no_stops\"]]\n",
    "        \n",
    "    df['question1_Lemmatization']  = [lemm(quest) for quest in df[\"question1_no_stops\"]]\n",
    "    df['question2_Lemmatization']  = [lemm(quest) for quest in df[\"question2_no_stops\"]]\n",
    "    \n",
    "    if save==True:\n",
    "        df.to_csv('preprocessed_Quora_full_train_data.csv')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc = preprocess(full_train_df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question1_no_stops</th>\n",
       "      <th>question2_no_stops</th>\n",
       "      <th>question1_Porter_stemmed</th>\n",
       "      <th>question2_Porter_stemmed</th>\n",
       "      <th>question1_Lemmatization</th>\n",
       "      <th>question2_Lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>share market invest india guide step</td>\n",
       "      <td>share market invest guide step</td>\n",
       "      <td>share market invest india guid step</td>\n",
       "      <td>share market invest guid step</td>\n",
       "      <td>share market invest india guide step</td>\n",
       "      <td>share market invest guide step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the story of kohinoor koh i noor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>diamond story noor koh kohinoor</td>\n",
       "      <td>stole diamond indian would happen noor koh gov...</td>\n",
       "      <td>diamond stori noor koh kohinoor</td>\n",
       "      <td>stole diamond indian would happen noor koh gov...</td>\n",
       "      <td>diamond story noor koh kohinoor</td>\n",
       "      <td>stole diamond indian would happen noor koh gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>vpn using speed connection internet increase</td>\n",
       "      <td>dns speed increased internet hacking</td>\n",
       "      <td>vpn use speed connect internet increas</td>\n",
       "      <td>dn speed increas internet hack</td>\n",
       "      <td>vpn using speed connection internet increase</td>\n",
       "      <td>dns speed increased internet hacking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math 23  24  math is d...</td>\n",
       "      <td>mentally solve lonely</td>\n",
       "      <td>find 23 remainder math divided 24</td>\n",
       "      <td>mental solv lone</td>\n",
       "      <td>find 23 remaind math divid 24</td>\n",
       "      <td>mentally solve lonely</td>\n",
       "      <td>find 23 remainder math divided 24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>which one dissolve in water quickly sugar  sal...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>oxide di one carbon methane water quickly diss...</td>\n",
       "      <td>fish survive would water salt</td>\n",
       "      <td>oxid di one carbon methan water quickli dissol...</td>\n",
       "      <td>fish surviv would water salt</td>\n",
       "      <td>oxide di one carbon methane water quickly diss...</td>\n",
       "      <td>fish survive would water salt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what is the story of kohinoor koh i noor diamond    \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3  why am i mentally very lonely how can i solve it    \n",
       "4  which one dissolve in water quickly sugar  sal...   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what would happen if the indian government sto...   \n",
       "2  how can internet speed be increased by hacking...   \n",
       "3  find the remainder when math 23  24  math is d...   \n",
       "4            which fish would survive in salt water    \n",
       "\n",
       "                                  question1_no_stops  \\\n",
       "0               share market invest india guide step   \n",
       "1                    diamond story noor koh kohinoor   \n",
       "2       vpn using speed connection internet increase   \n",
       "3                              mentally solve lonely   \n",
       "4  oxide di one carbon methane water quickly diss...   \n",
       "\n",
       "                                  question2_no_stops  \\\n",
       "0                     share market invest guide step   \n",
       "1  stole diamond indian would happen noor koh gov...   \n",
       "2               dns speed increased internet hacking   \n",
       "3                  find 23 remainder math divided 24   \n",
       "4                      fish survive would water salt   \n",
       "\n",
       "                            question1_Porter_stemmed  \\\n",
       "0               share market invest india guid step    \n",
       "1                   diamond stori noor koh kohinoor    \n",
       "2            vpn use speed connect internet increas    \n",
       "3                                  mental solv lone    \n",
       "4  oxid di one carbon methan water quickli dissol...   \n",
       "\n",
       "                            question2_Porter_stemmed  \\\n",
       "0                     share market invest guid step    \n",
       "1  stole diamond indian would happen noor koh gov...   \n",
       "2                    dn speed increas internet hack    \n",
       "3                     find 23 remaind math divid 24    \n",
       "4                      fish surviv would water salt    \n",
       "\n",
       "                             question1_Lemmatization  \\\n",
       "0              share market invest india guide step    \n",
       "1                   diamond story noor koh kohinoor    \n",
       "2      vpn using speed connection internet increase    \n",
       "3                             mentally solve lonely    \n",
       "4  oxide di one carbon methane water quickly diss...   \n",
       "\n",
       "                             question2_Lemmatization  \n",
       "0                    share market invest guide step   \n",
       "1  stole diamond indian would happen noor koh gov...  \n",
       "2              dns speed increased internet hacking   \n",
       "3                 find 23 remainder math divided 24   \n",
       "4                     fish survive would water salt   "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting empty questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_len(question):\n",
    "    num_words = len(question.split())\n",
    "    return num_words\n",
    "\n",
    "#Dropping all the sample where there is not one of the two questions\n",
    "len_1   = full_train_proc.question1_cleaned.apply(lambda x: cont_len(x))\n",
    "empty_1 = len_1[len_1==0].index\n",
    "\n",
    "len_2   = full_train_proc.question2_cleaned.apply(lambda x: cont_len(x))\n",
    "empty_2 = len_2[len_2==0].index\n",
    "\n",
    "to_delete = (empty_2).union(set(empty_1))\n",
    "\n",
    "full_train_proc = full_train_proc.drop(index=to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using CountVectorizer Solution\n",
    "### Naive CountVectorizer and attempts to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:21:25.113927Z",
     "start_time": "2021-03-15T16:21:19.032577Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english', strip_accents='unicode')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer='word', binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 stop_words='english', #I also have a list of more advanced stopwords we could use\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 #token_pattern='(?u)\\b\\w\\w+\\b', #not sure why it doesn't work with the default option\n",
    "                                                                 preprocessor=None, tokenizer=None, vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function stemmed_words at 0x7f86a4e875e0>,\n",
       "                stop_words='english', strip_accents='unicode')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#advanced version with stemming\n",
    "#this implemenetation failed because stop_words cannot be used if analyzer!='word'\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer=stemmed_words, binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 stop_words='english', #I also have a list of more advanced stopwords we could use\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 #token_pattern='(?u)\\b\\w\\w+\\b', #not sure why it doesn't work with the default option\n",
    "                                                                 preprocessor=None, tokenizer=None, vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:504: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function stemmed_words at 0x7f86a4e87700>,\n",
       "                stop_words='english', strip_accents='unicode',\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x7f8710b9d100>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#advanced version with lemmatization on top of stemming\n",
    "#this implemenetation failed because the tokenizer cannot be used if analyzer!='word'\n",
    "#additionally, if tokenizer!=None token_pattern and stop_words cannot be used\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer=stemmed_words, binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 stop_words='english', #I also have a list of more advanced stopwords we could use\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 #token_pattern='(?u)\\b\\w\\w+\\b', #not sure why it doesn't work with the default option\n",
    "                                                                 preprocessor=None, \n",
    "                                                                 tokenizer=LemmaTokenizer(), \n",
    "                                                                 vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with the improvement attempts so far\n",
    "\n",
    "* if analyzer=stemmed_words --> stop_words and tokenizer cannot be used\n",
    "* if tokenizer=LemmaTokenizer() --> token_pattern and stop_words cannot be used\n",
    "\n",
    "To solve these issues, we will not use the analyzer. We tokenize, remove the punctuation and stopwords, apply the regEx, and stem all inside the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final CountVectorizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "\n",
    "stopwords_list=nltk.corpus.stopwords.words('english')\n",
    "stops = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n",
    "stopwords_punctuation = (stopwords_list + list(set(stops) - set(stopwords_list)) + list(string.punctuation))\n",
    "\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = stemmer\n",
    "        self.lemmatizer = lemmatizer\n",
    "    def __call__(self, articles):\n",
    "        tokens = re.compile(r'(?u)\\b\\w\\w+\\b').findall(articles) \n",
    "        tokens = [' '.join(w for w in token.split() if w.lower() not in stopwords_punctuation) for token in tokens]\n",
    "        tokens_stem = [self.stemmer.stem(t) for t in tokens]\n",
    "        tokens_lem = [self.lemmatizer.lemmatize(t) for t in tokens_stem]\n",
    "        return tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(strip_accents='unicode',\n",
       "                tokenizer=<__main__.Tokenizer object at 0x7fe9f87e2e80>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer='word', binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 preprocessor=None, \n",
    "                                                                 tokenizer=Tokenizer(), \n",
    "                                                                 vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Other Features Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These functions generate new feature in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding len Words in common feat\n",
    "def len_common(q1, q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1.intersection(q2))\n",
    "\n",
    "#Adding len common words in common feat\n",
    "def len_not_common(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1 ^ q2)\n",
    "\n",
    "#Adding mean distance between common words \n",
    "def mean_dist_not_com(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    not_comm1 = (q1 ^ q2) - q1\n",
    "    if len(not_comm1)==0 : not_comm1={''}\n",
    "    not_comm2 = (q1 ^ q2) - q2\n",
    "    if len(not_comm2)==0 : not_comm2={''}\n",
    "    return statistics.mean([editdistance.eval(i[0],i[1]) for i in itertools.product(not_comm1, not_comm2)])\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString)) \n",
    "def both_number(q1,q2):\n",
    "    return hasNumbers(q1) *  hasNumbers(q2) \n",
    "\n",
    "# get average number of words\n",
    "def average_len(question):\n",
    "    num_words = len(question.split())\n",
    "    return len(question)/num_words   \n",
    "\n",
    "# levenshtein distance (for strings of unequal length)\n",
    "def levenshtein(q1, q2): \n",
    "    #create initial array (two for loops since q1 and q2 can differ in length)\n",
    "    dist_array = []\n",
    "    for i in range(len(q1)+1):\n",
    "        dist_array.append([0]*(len(q2)+1))\n",
    "        dist_array[i][0] = i\n",
    "    for j in range(len(q2)+1):\n",
    "        dist_array[0][j] = j\n",
    "\n",
    "    dist = [0]*3\n",
    "    for i in range(1,len(q1)+1):\n",
    "        for j in range(1,len(q2)+1):\n",
    "            dist[0] = dist_array[i-1][j-1] if q1[i-1]==q2[j-1] else dist_array[i-1][j-1]+1\n",
    "            dist[1] = dist_array[i][j-1]+1\n",
    "            dist[2] = dist_array[i-1][j]+1\n",
    "            dist_array[i][j]=min(dist)\n",
    "    \n",
    "    return dist_array[i][j]\n",
    "\n",
    "# self implemented jaccard similarity\n",
    "#from math import*\n",
    "def jaccard_similarity(vector1,vector2):\n",
    "    jacc_num = 0 \n",
    "    jacc_den = 0 \n",
    "    for index in enumerate(vector1): \n",
    "        if vector1[index] != 0 or vector2[index] != 0: \n",
    "            jacc_den += max(vector1[index], vector2[index]) \n",
    "            jacc_num += min(vector1[index], vector2[index]) \n",
    "    return jacc_num / jacc_den\n",
    "\n",
    "def common_tokens(string_1,string_2):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    \n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    x = nltk.word_tokenize(string_1)\n",
    "    y = nltk.word_tokenize(string_2)   \n",
    " \n",
    "    common_tokens = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common_tokens)\n",
    "\n",
    "def common_count(string_1,string_2, word_type = \"NOUN\"):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    tagged_1 = nltk.pos_tag(nltk.word_tokenize(string_1),tagset='universal')\n",
    "    tagged_2 = nltk.pos_tag(nltk.word_tokenize(string_2),tagset='universal')\n",
    "    \n",
    "    x = list([])\n",
    "    for word in tagged_1:\n",
    "        if word[1] == word_type:\n",
    "            x.append(word[0])\n",
    "                     \n",
    "    y = list([])\n",
    "    for word in tagged_2:\n",
    "        if word[1] == word_type:\n",
    "            y.append(word[0])\n",
    " \n",
    "    common = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common)\n",
    "\n",
    "def length_diff_characters(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in character length of two questions\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    y = str(y)\n",
    "    return abs(len(y)-len(x))\n",
    "\n",
    "def length_diff_tokens(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in number of tokens in two questions\n",
    "    \"\"\"\n",
    "    return abs(len(nltk.word_tokenize(y))-len(nltk.word_tokenize(x)))\n",
    "\n",
    "def common_numbers(x,y):\n",
    "    \"\"\"\n",
    "    count numbers present in both tokens\n",
    "    \"\"\"\n",
    "\n",
    "    x_numbers = re.findall(r'\\b\\d+\\b', x)\n",
    "    y_numbers = re.findall(r'\\b\\d+\\b', y)\n",
    "    \n",
    "    common_numbers = len(list(set(x_numbers).intersection(y_numbers)))\n",
    "    return(common_numbers)\n",
    "\n",
    "def first_word(x,y):\n",
    "    #first word\n",
    "    word_list_x = x.split()  # list of words\n",
    "    word_list_y = y.split()  # list of words\n",
    "    return word_list_x[0] == word_list_y[0]\n",
    "\n",
    "def last_word(x,y):\n",
    "    #last word\n",
    "    word_list_x = x.split()  # list of words\n",
    "    word_list_y = y.split()  # list of words\n",
    "    return word_list_x[-1] == word_list_y[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_train_proc_plus_feat['first_word'] = (full_train_proc_plus_feat.apply(lambda full_train_proc_plus_feat: first_word(full_train_proc_plus_feat.question1_cleaned, full_train_proc_plus_feat.question2_cleaned), axis=1))*1\n",
    "#full_train_proc_plus_feat['last_word'] = (full_train_proc_plus_feat.apply(lambda full_train_proc_plus_feat: last_word(full_train_proc_plus_feat.question1_cleaned, full_train_proc_plus_feat.question2_cleaned), axis=1))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/pablogranatiero/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "full_train_proc_plus_feat[\"length_diff_characters\"] = full_train_proc_plus_feat.apply(lambda full_train_proc_plus_feat: length_diff_characters(full_train_proc_plus_feat.question1_cleaned, full_train_proc_plus_feat.question2_cleaned), axis=1)\n",
    "full_train_proc_plus_feat['length_diff_characters'] = scaler.fit_transform(full_train_proc_plus_feat[['length_diff_characters']])\n",
    "print('done')\n",
    "full_train_proc_plus_feat[\"length_diff_tokens\"] = full_train_proc_plus_feat.apply(lambda full_train_proc_plus_feat: length_diff_tokens(full_train_proc_plus_feat.question1_cleaned, full_train_proc_plus_feat.question2_cleaned), axis=1)\n",
    "full_train_proc_plus_feat['length_diff_tokens'] = scaler.fit_transform(full_train_proc_plus_feat[['length_diff_tokens']])\n",
    "print('done')\n",
    "full_train_proc_plus_feat[\"common_numbers\"] = full_train_proc_plus_feat.apply(lambda full_train_proc_plus_feat: length_diff_characters(full_train_proc_plus_feat.question1_cleaned, full_train_proc_plus_feat.question2_cleaned), axis=1)\n",
    "full_train_proc_plus_feat['common_numbers'] = scaler.fit_transform(full_train_proc_plus_feat[['common_numbers']])\n",
    "\n",
    "full_train_proc_plus_feat.to_csv('features_added_Quora_full_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_features(df, scaler, col1, col2, save=False):\n",
    "    df = df.copy()\n",
    "    col1_to_transform = list(full_train_proc.columns).index(col1)\n",
    "    col2_to_transform = list(full_train_proc.columns).index(col2)\n",
    "\n",
    "    df['len_q1'] = [len(s) for s in df[col1]] \n",
    "    df['len_q2'] = [len(s) for s in df[col2]]\n",
    "    df['len_q1'] = scaler.fit_transform(df[['len_q1']])\n",
    "    df['len_q2'] = scaler.fit_transform(df[['len_q2']])\n",
    "    \n",
    "    df['len_common'] = [len_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_common'] = scaler.fit_transform(df[['len_common']])\n",
    "\n",
    "    df['len_not_common'] = [len_not_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_not_common'] = scaler.fit_transform(df[['len_not_common']])\n",
    "    \n",
    "    df['mean_dist_not_com'] = [mean_dist_not_com(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['mean_dist_not_com'] = scaler.fit_transform(df[['mean_dist_not_com']])\n",
    "    \n",
    "    df['both_number'] = [both_number(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    \n",
    "    df[\"avg_len_q1\"]= df[col1].apply(lambda x: average_len(x))\n",
    "    df[\"avg_len_q2\"]= df[col2].apply(lambda x: average_len(x))\n",
    "    df['avg_len_q1'] = scaler.fit_transform(df[['avg_len_q1']])\n",
    "    df['avg_len_q2'] = scaler.fit_transform(df[['avg_len_q2']])\n",
    "\n",
    "    df['edit_distance'] = [editdistance.eval(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['edit_distance'] = scaler.fit_transform(df[['edit_distance']])\n",
    "    \n",
    "    df['levenshtein_distance'] = [levenshtein(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['levenshtein_distance'] = scaler.fit_transform(df[['levenshtein_distance']])\n",
    "    \n",
    "    #df['manhattan_distance'] = [manhattan(df.loc[i,col1_to_transform],df.loc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    #df['manhattan_distance'] = scaler.fit_transform(df[['manhattan_distance']])\n",
    "    df[\"common_nouns\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"NOUN\"), axis=1)\n",
    "    df['common_nouns'] = scaler.fit_transform(df[['common_nouns']])\n",
    "\n",
    "    df[\"common_verbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"VERB\"), axis=1)\n",
    "    df['common_verbs'] = scaler.fit_transform(df[['common_verbs']])\n",
    "    \n",
    "    df[\"common_adjectives\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADJ\"), axis=1)\n",
    "    df['common_adjectives'] = scaler.fit_transform(df[['common_adjectives']])\n",
    "    \n",
    "    df[\"common_adverbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADV\"), axis=1)\n",
    "    df['common_adverbs'] = scaler.fit_transform(df[['common_adverbs']])\n",
    "\n",
    "    df[\"common_tokens\"] = df.apply(lambda df: common_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['common_tokens'] = scaler.fit_transform(df[['common_tokens']])\n",
    "\n",
    "    df[\"length_diff_characters\"] = df.apply(lambda df: length_diff_characters(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_characters'] = scaler.fit_transform(df[['length_diff_characters']])\n",
    "\n",
    "    df[\"length_diff_tokens\"] = df.apply(lambda df: length_diff_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_tokens'] = scaler.fit_transform(df[['length_diff_tokens']])\n",
    "\n",
    "    df[\"common_numbers\"] = df.apply(lambda df: length_diff_characters(df.question1, df.question2), axis=1)\n",
    "    df['common_numbers'] = scaler.fit_transform(df[['common_numbers']])\n",
    "\n",
    "    df['first_word'] = df.apply(lambda df: first_word(df[col1], df[col2]), axis=1)*1\n",
    "    df['last_word'] = df.apply(lambda df: last_word(df[col1], df[col2]), axis=1)*1\n",
    "    \n",
    "    if save==True:\n",
    "        df.to_csv('features_added_Quora_full_train_data.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404269, 26)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_proc_plus_feat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question1_no_stops</th>\n",
       "      <th>question2_no_stops</th>\n",
       "      <th>...</th>\n",
       "      <th>len_common</th>\n",
       "      <th>len_not_common</th>\n",
       "      <th>mean_dist_not_com</th>\n",
       "      <th>both_number</th>\n",
       "      <th>avg_len_q1</th>\n",
       "      <th>avg_len_q2</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>first_word</th>\n",
       "      <th>last_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>share market invest india guide step</td>\n",
       "      <td>share market invest guide step</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.167910</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the story of kohinoor koh i noor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>diamond story noor koh kohinoor</td>\n",
       "      <td>stole diamond indian would happen noor koh gov...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278571</td>\n",
       "      <td>0.211940</td>\n",
       "      <td>0.038356</td>\n",
       "      <td>0.038356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>vpn using speed connection internet increase</td>\n",
       "      <td>dns speed increased internet hacking</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.204321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301020</td>\n",
       "      <td>0.219403</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math 23  24  math is d...</td>\n",
       "      <td>mentally solve lonely</td>\n",
       "      <td>find 23 remainder math divided 24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.168889</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.168772</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>which one dissolve in water quickly sugar  sal...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>oxide di one carbon methane water quickly diss...</td>\n",
       "      <td>fish survive would water salt</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.198903</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.204691</td>\n",
       "      <td>0.049315</td>\n",
       "      <td>0.049315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what is the story of kohinoor koh i noor diamond    \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3  why am i mentally very lonely how can i solve it    \n",
       "4  which one dissolve in water quickly sugar  sal...   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what would happen if the indian government sto...   \n",
       "2  how can internet speed be increased by hacking...   \n",
       "3  find the remainder when math 23  24  math is d...   \n",
       "4            which fish would survive in salt water    \n",
       "\n",
       "                                  question1_no_stops  \\\n",
       "0               share market invest india guide step   \n",
       "1                    diamond story noor koh kohinoor   \n",
       "2       vpn using speed connection internet increase   \n",
       "3                              mentally solve lonely   \n",
       "4  oxide di one carbon methane water quickly diss...   \n",
       "\n",
       "                                  question2_no_stops  ... len_common  \\\n",
       "0                     share market invest guide step  ...      0.275   \n",
       "1  stole diamond indian would happen noor koh gov...  ...      0.175   \n",
       "2               dns speed increased internet hacking  ...      0.100   \n",
       "3                  find 23 remainder math divided 24  ...      0.000   \n",
       "4                      fish survive would water salt  ...      0.100   \n",
       "\n",
       "  len_not_common mean_dist_not_com both_number  avg_len_q1  avg_len_q2  \\\n",
       "0       0.007576          0.185185           0    0.265306    0.167910   \n",
       "1       0.075758          0.190476           0    0.278571    0.211940   \n",
       "2       0.121212          0.204321           0    0.301020    0.219403   \n",
       "3       0.151515          0.168889           0    0.246753    0.168772   \n",
       "4       0.090909          0.198903           0    0.351648    0.204691   \n",
       "\n",
       "   edit_distance  levenshtein_distance  first_word  last_word  \n",
       "0       0.008219              0.008219           1          0  \n",
       "1       0.038356              0.038356           1          0  \n",
       "2       0.036530              0.036530           1          0  \n",
       "3       0.044749              0.044749           0          0  \n",
       "4       0.049315              0.049315           1          0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_proc_plus_feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "full_train_proc_plus_feat = Add_features(full_train_proc, scaler, 'question1_cleaned', 'question2_cleaned', save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc_plus_feat.to_csv('features_added_Quora_full_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Two Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 2:  `get_features_from_df`\n",
    "\n",
    "Make a function `get_features_from_df` that given a dataframe containing the format of the train data\n",
    "it returns a scipy sparse matrix with the features from question 1 and question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_on_q1_q2(df, model):\n",
    "    q_list1 = list(df[\"question1_cleaned\"])\n",
    "    q_list2 = list(df[\"question2_cleaned\"])\n",
    "    all_questions = q_list1 + q_list2 \n",
    "    model.fit(all_questions)\n",
    "    return\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    ############### Begin exercise ###################\n",
    "    # what is kaggle                  q1\n",
    "    # What is the kaggle platform     q2\n",
    "    X_q1 = count_vectorizer.transform(q1_casted)\n",
    "    X_q2 = count_vectorizer.transform(q2_casted)    \n",
    "    X_q1q2 = scipy.sparse.hstack((X_q1,X_q2))\n",
    "    ############### End exercise ###################\n",
    "\n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    #svm_model = svm.SVC()\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    #svm_model.fit(X_tr_q1q2, y_train)\n",
    "                                                   \n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_train, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_train, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "                                              \n",
    "                                                       \n",
    "                                                       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8560015673032263, logistic_val_acc:0.7953934745032775, logistic_test_acc:0.7956985843493918\n",
      "xgb_train_acc:0.8659191549450259, xgb_val_acc:0.8093737107889942, xgb_test_acc:0.8097783288425393\n"
     ]
    }
   ],
   "source": [
    "logistic_simple, xgb_model_simple, acc_logistic_simple, acc_xgboost_simple = \\\n",
    "train_models(full_train_proc_plus_feat, get_features_from_df, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:59:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8416865189699401, logistic_val_acc:0.7958238445455491, logistic_test_acc:0.797239576925177\n",
      "xgb_train_acc:0.9070173792036841, xgb_val_acc:0.8234103363606295, xgb_test_acc:0.827242512987826\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. More complex models - Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=True):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:56:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9074969597126898, logistic_val_acc:0.8726778599886169, logistic_test_acc:0.8772466341269656\n",
      "xgb_train_acc:0.9766716766767485, xgb_val_acc:0.8987541590750521, xgb_test_acc:0.9019259149390173\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf_sim, xgb_model_simple_tf_sim, acc_logistic_tf_sim, acc_xgboost_tf_sim = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train_val_test(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    return  train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_a_column_feat(data, col, sparse_matrix):\n",
    "    feat_q = data[col].to_numpy().reshape(len(data[col]),1)\n",
    "        \n",
    "    return scipy.sparse.hstack((sparse_matrix,feat_q)).tocsr()        \n",
    "\n",
    "def train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, col_list):    \n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    \n",
    "    for col in col_list:\n",
    "        X_train_q1q2      = add_a_column_feat(train_df, col, X_train_q1q2)\n",
    "        X_val_q1q2        = add_a_column_feat(val_df, col, X_val_q1q2)\n",
    "        X_test_q1q2       = add_a_column_feat(test_df, col, X_test_q1q2)\n",
    "    print('...features added...')\n",
    "\n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    print('...logistic model fitted...')\n",
    "\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    print('...model fitted...')\n",
    "\n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_val, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_test, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2 = vectorize_train_val_test(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:10:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9105569629864025, logistic_val_acc:0.877148443480405, logistic_test_acc:0.8810641865271815\n",
      "xgb_train_acc:0.9893273664276654, xgb_val_acc:0.9060839423181429, xgb_test_acc:0.906456667632005\n"
     ]
    }
   ],
   "source": [
    "cols_lenght = ['len_q1', 'len_q2', 'len_common', 'len_not_common', 'avg_len_q1', 'avg_len_q2']\n",
    "logistic_simple_tf_sim_len, xgb_model_simple_tf_sim_len, acc_logistic_tfidf_sim_len, acc_xgboost_tfidf_sim_len \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, cols_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2 = vectorize_train_val_test(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...features added...\n",
      "...logistic model fitted...\n",
      "[22:45:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:45:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "...model fitted...\n",
      "logistic_train_acc:0.9122547681146612, logistic_val_acc:0.879432834809194, logistic_test_acc:0.8829894326066793\n",
      "xgb_train_acc:0.991570516727775, xgb_val_acc:0.9099064872805638, xgb_test_acc:0.9095038783787787\n"
     ]
    }
   ],
   "source": [
    "cols_dist = cols_lenght + ['mean_dist_not_com', 'edit_distance', 'levenshtein_distance']\n",
    "\n",
    "logistic_simple_tf_sim_len_dist, xgb_model_simple_tf_sim_len_dist, acc_logistic_tfidf_sim_len_dist, acc_xgboost_tfidf_sim_len_dist \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, cols_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...features added...\n",
      "...logistic model fitted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:50:46] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:51:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "...model fitted...\n",
      "logistic_train_acc:0.9160962845200364, logistic_val_acc:0.8843373353169895, logistic_test_acc:0.8881570223833523\n",
      "xgb_train_acc:0.9933322981345474, xgb_val_acc:0.9168239801135734, xgb_test_acc:0.9158050435110939\n"
     ]
    }
   ],
   "source": [
    "all_cols = cols_dist + ['both_number', 'first_word', 'last_word', 'length_diff_characters', 'length_diff_tokens', 'common_numbers']\n",
    "\n",
    "logistic_simple_tf_sim_len_dist_extra, xgb_model_simple_tf_sim_len_dist_extra, acc_logistic_tfidf_sim_len_dist_extra, acc_xgboost_tfidf_sim_len_dist_extra \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, all_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame({'BOW_No_Feat_Logistic':acc_logistic_simple,'BOW_No_Feat_XGB':acc_xgboost_simple,\n",
    "                     'TFIDF_No_Feat_Logistic':acc_logistic_simple_tf,'TFIDF_No_Feat_XGB':acc_xgboost_simple_tf,\n",
    "                     'TFIDF_Cos_Sim_Logistic':acc_logistic_tf_sim,'TFIDF_Cos_Sim_XGB':acc_xgboost_tf_sim,\n",
    "                     'TFIDF_Cos_Sim_Len_Logistic':acc_logistic_tfidf_sim_len,'TFIDF_Cos_Sim_Len_XGB':acc_xgboost_tfidf_sim_len,\n",
    "                     'TFIDF_Cos_Sim_Len_Dist_Logistic':acc_logistic_tfidf_sim_len_dist,'TFIDF_Cos_Sim_Len_Dist_XGB':acc_xgboost_tfidf_sim_len_dist,\n",
    "                     'TFIDF_Cos_Sim_Len_Dist_ExtraFeat_Logistic':acc_logistic_tfidf_sim_len_dist_extra,'TFIDF_Cos_Sim_Len_Dist_ExtraFeat_XGB':acc_xgboost_tfidf_sim_len_dist_extra,\n",
    "                    }, index=['Train','Validation', 'Test'])\n",
    "hist.to_csv('hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW_No_Feat_Logistic</th>\n",
       "      <th>BOW_No_Feat_XGB</th>\n",
       "      <th>TFIDF_No_Feat_Logistic</th>\n",
       "      <th>TFIDF_No_Feat_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_XGB</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_ExtraFeat_Logistic</th>\n",
       "      <th>TFIDF_Cos_Sim_Len_Dist_ExtraFeat_XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.856002</td>\n",
       "      <td>0.865919</td>\n",
       "      <td>0.841687</td>\n",
       "      <td>0.907017</td>\n",
       "      <td>0.907497</td>\n",
       "      <td>0.976672</td>\n",
       "      <td>0.910557</td>\n",
       "      <td>0.989327</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>0.991571</td>\n",
       "      <td>0.916096</td>\n",
       "      <td>0.993332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.795393</td>\n",
       "      <td>0.809374</td>\n",
       "      <td>0.795824</td>\n",
       "      <td>0.823410</td>\n",
       "      <td>0.872678</td>\n",
       "      <td>0.898754</td>\n",
       "      <td>0.877148</td>\n",
       "      <td>0.906084</td>\n",
       "      <td>0.879433</td>\n",
       "      <td>0.909906</td>\n",
       "      <td>0.884337</td>\n",
       "      <td>0.916824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.809778</td>\n",
       "      <td>0.797240</td>\n",
       "      <td>0.827243</td>\n",
       "      <td>0.877247</td>\n",
       "      <td>0.901926</td>\n",
       "      <td>0.881064</td>\n",
       "      <td>0.906457</td>\n",
       "      <td>0.882989</td>\n",
       "      <td>0.909504</td>\n",
       "      <td>0.888157</td>\n",
       "      <td>0.915805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BOW_No_Feat_Logistic  BOW_No_Feat_XGB  TFIDF_No_Feat_Logistic  \\\n",
       "Train                   0.856002         0.865919                0.841687   \n",
       "Validation              0.795393         0.809374                0.795824   \n",
       "Test                    0.795699         0.809778                0.797240   \n",
       "\n",
       "            TFIDF_No_Feat_XGB  TFIDF_Cos_Sim_Logistic  TFIDF_Cos_Sim_XGB  \\\n",
       "Train                0.907017                0.907497           0.976672   \n",
       "Validation           0.823410                0.872678           0.898754   \n",
       "Test                 0.827243                0.877247           0.901926   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Logistic  TFIDF_Cos_Sim_Len_XGB  \\\n",
       "Train                         0.910557               0.989327   \n",
       "Validation                    0.877148               0.906084   \n",
       "Test                          0.881064               0.906457   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Dist_Logistic  TFIDF_Cos_Sim_Len_Dist_XGB  \\\n",
       "Train                              0.912255                    0.991571   \n",
       "Validation                         0.879433                    0.909906   \n",
       "Test                               0.882989                    0.909504   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Dist_ExtraFeat_Logistic  \\\n",
       "Train                                        0.916096   \n",
       "Validation                                   0.884337   \n",
       "Test                                         0.888157   \n",
       "\n",
       "            TFIDF_Cos_Sim_Len_Dist_ExtraFeat_XGB  \n",
       "Train                                   0.993332  \n",
       "Validation                              0.916824  \n",
       "Test                                    0.915805  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all in string\n",
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    #assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    mylist_of_strings = []\n",
    "    for x in mylist:\n",
    "        mylist_of_strings.append(str(x))\n",
    "\n",
    "    return mylist_of_strings\n",
    "\n",
    "#Removing Stops Words\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    stops     = set(stopwords.words(\"english\"))\n",
    "    token_doc = set(word_tokenize(doc))\n",
    "    token_doc = list(token_doc - stops)\n",
    "    return \" \".join(token_doc)\n",
    "\n",
    "#Adding len Words in common feat\n",
    "def len_common(q1, q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1.intersection(q2))\n",
    "\n",
    "#Adding len common words in common feat\n",
    "def len_not_common(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1 ^ q2)\n",
    "\n",
    "#Adding mean distance between common words \n",
    "def mean_dist_not_com(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    not_comm1 = (q1 ^ q2) - q1\n",
    "    if len(not_comm1)==0 : not_comm1={''}\n",
    "    not_comm2 = (q1 ^ q2) - q2\n",
    "    if len(not_comm2)==0 : not_comm2={''}\n",
    "    return statistics.mean([editdistance.eval(i[0],i[1]) for i in itertools.product(not_comm1, not_comm2)])\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString)) \n",
    "def both_number(q1,q2):\n",
    "    return hasNumbers(q1) *  hasNumbers(q2) \n",
    "\n",
    "# get average number of words\n",
    "def average_len(question):\n",
    "    num_words = len(question.split())\n",
    "    return len(question)/num_words   \n",
    "\n",
    "# levenshtein distance (for strings of unequal length)\n",
    "def levenshtein(q1, q2): \n",
    "    #create initial array (two for loops since q1 and q2 can differ in length)\n",
    "    dist_array = []\n",
    "    for i in range(len(q1)+1):\n",
    "        dist_array.append([0]*(len(q2)+1))\n",
    "        dist_array[i][0] = i\n",
    "    for j in range(len(q2)+1):\n",
    "        dist_array[0][j] = j\n",
    "\n",
    "    dist = [0]*3\n",
    "    for i in range(1,len(q1)+1):\n",
    "        for j in range(1,len(q2)+1):\n",
    "            dist[0] = dist_array[i-1][j-1] if q1[i-1]==q2[j-1] else dist_array[i-1][j-1]+1\n",
    "            dist[1] = dist_array[i][j-1]+1\n",
    "            dist[2] = dist_array[i-1][j]+1\n",
    "            dist_array[i][j]=min(dist)\n",
    "    \n",
    "    return dist_array[i][j]\n",
    "\n",
    "# self implemented jaccard similarity\n",
    "#from math import*\n",
    " \n",
    "def jaccard_similarity(vector1,vector2):\n",
    "    jacc_num = 0 \n",
    "    jacc_den = 0 \n",
    "    for index in enumerate(vector1): \n",
    "        if vector1[index] != 0 or vector2[index] != 0: \n",
    "            jacc_den += max(vector1[index], vector2[index]) \n",
    "            jacc_num += min(vector1[index], vector2[index]) \n",
    "    return jacc_num / jacc_den\n",
    "\n",
    "def common_tokens(string_1,string_2):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    \n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    x = nltk.word_tokenize(string_1)\n",
    "    y = nltk.word_tokenize(string_2)   \n",
    " \n",
    "    common_tokens = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common_tokens)\n",
    "\n",
    "def common_count(string_1,string_2, word_type = \"NOUN\"):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    tagged_1 = nltk.pos_tag(nltk.word_tokenize(string_1),tagset='universal')\n",
    "    tagged_2 = nltk.pos_tag(nltk.word_tokenize(string_2),tagset='universal')\n",
    "    \n",
    "    x = list([])\n",
    "    for word in tagged_1:\n",
    "        if word[1] == word_type:\n",
    "            x.append(word[0])\n",
    "                     \n",
    "    y = list([])\n",
    "    for word in tagged_2:\n",
    "        if word[1] == word_type:\n",
    "            y.append(word[0])\n",
    " \n",
    "    common = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common)\n",
    "\n",
    "def length_diff_characters(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in character length of two questions\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    y = str(y)\n",
    "    return abs(len(y)-len(x))\n",
    "\n",
    "def length_diff_tokens(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in number of tokens in two questions\n",
    "    \"\"\"\n",
    "    return abs(len(nltk.word_tokenize(y))-len(nltk.word_tokenize(x)))\n",
    "\n",
    "def common_numbers(x,y):\n",
    "    \"\"\"\n",
    "    count numbers present in both tokens\n",
    "    \"\"\"\n",
    "\n",
    "    x_numbers = re.findall(r'\\b\\d+\\b', x)\n",
    "    y_numbers = re.findall(r'\\b\\d+\\b', y)\n",
    "    \n",
    "    common_numbers = len(list(set(x_numbers).intersection(y_numbers)))\n",
    "    return(common_numbers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/pablogranatiero/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test[\"common_nouns\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"NOUN\"), axis=1)\n",
    "#df_test[\"common_verbs\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"VERB\"), axis=1)\n",
    "#df_test[\"common_adjectives\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"ADJ\"), axis=1)\n",
    "#df_test[\"common_adverbs\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"ADV\"), axis=1)\n",
    "#df_test[\"common_tokens\"] = df_test.apply(lambda df_test: common_tokens(df_test.question1, df_test.question2), axis=1)\n",
    "#df_test[\"length_diff_characters\"] = df_test.apply(lambda df_test: length_diff_characters(df_test.question1, df_test.question2), axis=1)\n",
    "#df_test[\"length_diff_tokens\"] = df_test.apply(lambda df_test: length_diff_tokens(df_test.question1, df_test.question2), axis=1)\n",
    "#df_test[\"common_numbers\"] = df_test.apply(lambda df_test: length_diff_characters(df_test.question1, df_test.question2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_features(df, scaler, col1, col2, save=False):\n",
    "    df = df.copy()\n",
    "    col1_to_transform = list(full_train_proc.columns).index(col1)\n",
    "    col2_to_transform = list(full_train_proc.columns).index(col2)\n",
    "\n",
    "    df['len_q1'] = [len(s) for s in df[col1]] \n",
    "    df['len_q2'] = [len(s) for s in df[col2]]\n",
    "    df['len_q1'] = scaler.fit_transform(df[['len_q1']])\n",
    "    df['len_q2'] = scaler.fit_transform(df[['len_q2']])\n",
    "    \n",
    "    df['len_common'] = [len_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_common'] = scaler.fit_transform(df[['len_common']])\n",
    "\n",
    "    df['len_not_common'] = [len_not_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_not_common'] = scaler.fit_transform(df[['len_not_common']])\n",
    "    \n",
    "    df['mean_dist_not_com'] = [mean_dist_not_com(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['mean_dist_not_com'] = scaler.fit_transform(df[['mean_dist_not_com']])\n",
    "    \n",
    "    df['both_number'] = [both_number(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    \n",
    "    df[\"avg_len_q1\"]= df[col1].apply(lambda x: average_len(x))\n",
    "    df[\"avg_len_q2\"]= df[col2].apply(lambda x: average_len(x))\n",
    "    df['avg_len_q1'] = scaler.fit_transform(df[['avg_len_q1']])\n",
    "    df['avg_len_q2'] = scaler.fit_transform(df[['avg_len_q2']])\n",
    "\n",
    "    df['edit_distance'] = [editdistance.eval(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['edit_distance'] = scaler.fit_transform(df[['edit_distance']])\n",
    "    \n",
    "    df['levenshtein_distance'] = [levenshtein(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['levenshtein_distance'] = scaler.fit_transform(df[['levenshtein_distance']])\n",
    "    \n",
    "    #df['manhattan_distance'] = [manhattan(df.loc[i,col1_to_transform],df.loc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    #df['manhattan_distance'] = scaler.fit_transform(df[['manhattan_distance']])\n",
    "    df[\"common_nouns\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"NOUN\"), axis=1)\n",
    "    df['common_nouns'] = scaler.fit_transform(df[['common_nouns']])\n",
    "\n",
    "    df[\"common_verbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"VERB\"), axis=1)\n",
    "    df['common_verbs'] = scaler.fit_transform(df[['common_verbs']])\n",
    "    \n",
    "    df[\"common_adjectives\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADJ\"), axis=1)\n",
    "    df['common_adjectives'] = scaler.fit_transform(df[['common_adjectives']])\n",
    "    \n",
    "    df[\"common_adverbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADV\"), axis=1)\n",
    "    df['common_adverbs'] = scaler.fit_transform(df[['common_adverbs']])\n",
    "\n",
    "    df[\"common_tokens\"] = df.apply(lambda df: common_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['common_tokens'] = scaler.fit_transform(df[['common_tokens']])\n",
    "\n",
    "    df[\"length_diff_characters\"] = df.apply(lambda df: length_diff_characters(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_characters'] = scaler.fit_transform(df[['length_diff_characters']])\n",
    "\n",
    "    df[\"length_diff_tokens\"] = df.apply(lambda df: length_diff_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_tokens'] = scaler.fit_transform(df[['length_diff_tokens']])\n",
    "\n",
    "    df[\"common_numbers\"] = df.apply(lambda df: length_diff_characters(df.question1, df.question2), axis=1)\n",
    "    df['common_numbers'] = scaler.fit_transform(df[['common_numbers']])\n",
    "\n",
    "    if save==True:\n",
    "        df.to_csv('features_added_Quora_full_train_data.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404269"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_proc_plus_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "full_train_proc_plus_feat = Add_features(full_train_proc, scaler, 'question1_cleaned', 'question2_cleaned', save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc_plus_feat.to_csv('features_added_Quora_full_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Two Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 2:  `get_features_from_df`\n",
    "\n",
    "Make a function `get_features_from_df` that given a dataframe containing the format of the train data\n",
    "it returns a scipy sparse matrix with the features from question 1 and question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_on_q1_q2(df, model):\n",
    "    q_list1 = list(df[\"question1_cleaned\"])\n",
    "    q_list2 = list(df[\"question2_cleaned\"])\n",
    "    all_questions = q_list1 + q_list2 \n",
    "    model.fit(all_questions)\n",
    "    return\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    ############### Begin exercise ###################\n",
    "    # what is kaggle                  q1\n",
    "    # What is the kaggle platform     q2\n",
    "    X_q1 = count_vectorizer.transform(q1_casted)\n",
    "    X_q2 = count_vectorizer.transform(q2_casted)    \n",
    "    X_q1q2 = scipy.sparse.hstack((X_q1,X_q2))\n",
    "    ############### End exercise ###################\n",
    "\n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    #svm_model = svm.SVC()\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    #svm_model.fit(X_tr_q1q2, y_train)\n",
    "                                                   \n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_train, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_train, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "                                              \n",
    "                                                       \n",
    "                                                       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8560015673032263, logistic_val_acc:0.7953934745032775, logistic_test_acc:0.7956985843493918\n",
      "xgb_train_acc:0.8659191549450259, xgb_val_acc:0.8093737107889942, xgb_test_acc:0.8097783288425393\n"
     ]
    }
   ],
   "source": [
    "logistic_simple, xgb_model_simple, acc_logistic_simple, acc_xgboost_simple = \\\n",
    "train_models(full_train_proc_plus_feat, get_features_from_df, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:59:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8416865189699401, logistic_val_acc:0.7958238445455491, logistic_test_acc:0.797239576925177\n",
      "xgb_train_acc:0.9070173792036841, xgb_val_acc:0.8234103363606295, xgb_test_acc:0.827242512987826\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. More complex models - Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=True):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:56:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9074969597126898, logistic_val_acc:0.8726778599886169, logistic_test_acc:0.8772466341269656\n",
      "xgb_train_acc:0.9766716766767485, xgb_val_acc:0.8987541590750521, xgb_test_acc:0.9019259149390173\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf_sim, xgb_model_simple_tf_sim, acc_logistic_tf_sim, acc_xgboost_tf_sim = \\\n",
    "train_models(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train_val_test(df, get_feat_model, vectorizer_func):\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    return  train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_a_column_feat(data, col, sparse_matrix):\n",
    "    feat_q = data[col].to_numpy().reshape(len(data[col]),1)\n",
    "        \n",
    "    return scipy.sparse.hstack((sparse_matrix,feat_q)).tocsr()        \n",
    "\n",
    "def train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, col_list):    \n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    \n",
    "    for col in col_list:\n",
    "        X_train_q1q2      = add_a_column_feat(train_df, col, X_train_q1q2)\n",
    "        X_val_q1q2        = add_a_column_feat(val_df, col, X_val_q1q2)\n",
    "        X_test_q1q2       = add_a_column_feat(test_df, col, X_test_q1q2)\n",
    "    print('...features added...')\n",
    "\n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    print('...logistic model fitted...')\n",
    "\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    print('...model fitted...')\n",
    "\n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_val, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_test, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2 = vectorize_train_val_test(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:10:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9105569629864025, logistic_val_acc:0.877148443480405, logistic_test_acc:0.8810641865271815\n",
      "xgb_train_acc:0.9893273664276654, xgb_val_acc:0.9060839423181429, xgb_test_acc:0.906456667632005\n"
     ]
    }
   ],
   "source": [
    "cols_lenght = ['len_q1', 'len_q2', 'len_common', 'len_not_common', 'avg_len_q1', 'avg_len_q2']\n",
    "logistic_simple_tf_sim_len, xgb_model_simple_tf_sim_len, acc_logistic_tfidf_sim_len, acc_xgboost_tfidf_sim_len \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, cols_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, X_train_q1q2, X_val_q1q2, X_test_q1q2 = vectorize_train_val_test(full_train_proc_plus_feat, get_tfidf, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...features added...\n",
      "...logistic model fitted...\n",
      "[22:45:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_dist = cols_lenght + ['mean_dist_not_com', 'edit_distance', 'levenshtein_distance']\n",
    "\n",
    "logistic_simple_tf_sim_len_dist, xgb_model_simple_tf_sim_len_dist, acc_logistic_tfidf_sim_len_dist, acc_xgboost_tfidf_sim_len_dist \\\n",
    " = train_models_plus_feat(X_train_q1q2, X_val_q1q2, X_test_q1q2, train_df, val_df, test_df, cols_dist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
