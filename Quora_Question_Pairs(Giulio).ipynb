{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing data & cast_list_as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:16:55.198893Z",
     "start_time": "2021-03-15T16:16:55.196839Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import statistics\n",
    "import editdistance\n",
    "import itertools  \n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:16:56.110580Z",
     "start_time": "2021-03-15T16:16:56.108671Z"
    }
   },
   "outputs": [],
   "source": [
    "cl = sklearn.linear_model.Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data =  os.path.expanduser('~') \n",
    "\n",
    "\n",
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(os.path.join(path_data,\n",
    "                                    os.path.join(\"Datasets\", \"kaggle_datasets\", \"quora\", \"quora_train_data.csv\")))\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(os.path.join(path_data,\n",
    "                                    os.path.join(\"Datasets\", \"kaggle_datasets\", \"quora\", \"test.csv\"))) # \"test.csv\" is the same as \"quora_test_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[3306,'question1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T12:54:11.684056Z",
     "start_time": "2020-02-25T12:54:11.681299Z"
    }
   },
   "source": [
    "###  Exercise 1:  `cast_list_as_strings`\n",
    "\n",
    "Build a function  **`cast_list_as_strings`** that casts each element in the input list to a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q = list(train_df[\"question1\"]) + list(train_df[\"question2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:54.544067Z",
     "start_time": "2021-03-15T16:20:54.406412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['float', 'str'], dtype='<U5')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_ = [type(q).__name__ for q in all_q]\n",
    "np.unique(types_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:55.642705Z",
     "start_time": "2021-03-15T16:20:55.639644Z"
    }
   },
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    #assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    mylist_of_strings = []\n",
    "    for x in mylist:\n",
    "        mylist_of_strings.append(str(x))\n",
    "\n",
    "    return mylist_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:57.439436Z",
     "start_time": "2021-03-15T16:20:57.263710Z"
    }
   },
   "outputs": [],
   "source": [
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "#q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "#q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:20:59.132475Z",
     "start_time": "2021-03-15T16:20:59.024672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['str'], dtype='<U3')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_ = [type(q).__name__ for q in q2_train]\n",
    "np.unique(types_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation, substituting and lowering\n",
    "Stolen from https://www.kaggle.com/benjaminkz/quora-question-pairs-data-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_transformation_remove_punctuation(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"when's\", \"when is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)  # 除了上面的特殊情况外，“\\'s”只能表示所有格，应替换成“ ”\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text)\n",
    "    text = re.sub(r\" ds \", \" data science \", text)\n",
    "    text = re.sub(r\" ee \", \" electronic engineering \", text)\n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iphone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the us\", \"america\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    \n",
    "    punctuation= '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "\n",
    "    text = \"\".join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = train_df['question1'][0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the step by step guide to invest in share market in india '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = common_words_transformation_remove_punctuation(doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    stops = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n",
    "    all_stopwords = (stopwords_list + list(set(stops) - set(stopwords_list)))\n",
    "    token_doc = set(word_tokenize(doc))\n",
    "    token_doc = list(token_doc - all_stopwords)\n",
    "    return \" \".join(token_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter    = PorterStemmer()\n",
    "#lancaster = LancasterStemmer()\n",
    "#Snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is creating different versions of the questions based on the different cleaning methods defined above.\n",
    "#once we understand which of these methods work best, we will combined them in question1_preprocessed and question2_preprocessed.\n",
    "\n",
    "def preprocess(df, save=False):\n",
    "    df = df.copy()\n",
    "    df['question1'] = cast_list_as_strings(list(df[\"question1\"]))\n",
    "    df['question2'] = cast_list_as_strings(list(df[\"question2\"]))\n",
    "        \n",
    "    df['question1_cleaned'] = [common_words_transformation_remove_punctuation(quest) for quest in df[\"question1\"]]\n",
    "    df['question2_cleaned'] = [common_words_transformation_remove_punctuation(quest) for quest in df[\"question2\"]]\n",
    "    \n",
    "    df['question1_noAccents'] = [remove_accented_chars(quest) for quest in df[\"question1\"]]\n",
    "    df['question2_noAccents'] = [remove_accented_chars(quest) for quest in df[\"question2\"]]\n",
    "    \n",
    "    df['question1_no_stops'] = [remove_stopwords(quest) for quest in df[\"question1_cleaned\"]]\n",
    "    df['question2_no_stops'] = [remove_stopwords(quest) for quest in df[\"question2_cleaned\"]]\n",
    "    \n",
    "    df['question1_Porter_stemmed'] = [stem(quest) for quest in df[\"question1_no_stops\"]]\n",
    "    df['question2_Porter_stemmed'] = [stem(quest) for quest in df[\"question2_no_stops\"]]\n",
    "        \n",
    "    df['question1_Lemmatization']  = [lemm(quest) for quest in df[\"question1_no_stops\"]]\n",
    "    df['question2_Lemmatization']  = [lemm(quest) for quest in df[\"question2_no_stops\"]]\n",
    "    \n",
    "    \n",
    "    if save==True:\n",
    "        df.to_csv('preprocessed_Quora_full_train_data.csv')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'set' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-96284beb5299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_train_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-339cd8b05c43>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(df, save)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2_noAccents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_accented_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1_no_stops'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question1_cleaned\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2_no_stops'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question2_cleaned\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-339cd8b05c43>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2_noAccents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_accented_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1_no_stops'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question1_cleaned\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2_no_stops'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question2_cleaned\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-21390bfe662e>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstopwords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"0o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"6b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"6o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ab\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"able\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"about\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"above\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"abst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ac\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accordance\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"according\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accordingly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"across\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"act\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"actually\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"added\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"af\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"affected\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"affecting\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"affects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"afterwards\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"again\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"against\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ah\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ain't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"al\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"allow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"allows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"almost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"alone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"along\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"already\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"also\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"although\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"always\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"am\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"among\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"amongst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"amoungst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"amount\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"an\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"and\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"announce\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"another\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"any\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anybody\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anyhow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anymore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anyone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anything\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anyway\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anyways\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anywhere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ao\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ap\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apart\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apparently\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"appear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"appreciate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"appropriate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"approximately\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"are\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aren\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aren't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"around\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aside\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"asking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"associated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"au\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"av\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"available\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"away\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"awfully\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ay\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"az\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ba\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"back\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"be\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"became\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"because\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"become\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"becomes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"becoming\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"been\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"before\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beforehand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beginning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beginnings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begins\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"behind\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"being\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"believe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"below\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beside\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"besides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"better\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"between\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beyond\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bill\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"biol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"both\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bottom\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"br\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"brief\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"briefly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"but\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"by\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"call\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"came\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"can\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cannot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"can't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cause\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"causes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ce\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"certain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"certainly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"changes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ci\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"clearly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c'mon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"co\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"com\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"come\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"comes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"con\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"concerning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"consequently\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"consider\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"considering\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"contain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"containing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"contains\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"corresponding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"could\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"couldn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"couldnt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"couldn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"course\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cry\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ct\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"currently\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"da\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"de\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"definitely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"describe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"described\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"despite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"di\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"did\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"didn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"didn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"different\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"do\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"does\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doesn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doesn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"don\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"don't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"down\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"downwards\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"du\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"due\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"during\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"e\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"e2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"e3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ea\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"each\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"edu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ee\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ef\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"effect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ei\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eighty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"either\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ej\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"el\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eleven\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"else\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"elsewhere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"em\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"empty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ending\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"enough\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entirely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"er\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"es\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"especially\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"est\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"et\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"et-al\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"etc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ev\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"even\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ever\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"every\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"everybody\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"everyone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"everything\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"everywhere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exactly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"example\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"except\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ey\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"far\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"few\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fifteen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fifth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fify\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fill\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"find\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fire\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"five\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"followed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"following\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"follows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"former\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"formerly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"found\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"four\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"front\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ft\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"further\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"furthermore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ga\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gave\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"getting\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"give\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"given\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gives\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"giving\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"go\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"goes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"going\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"got\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gotten\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"greetings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"h2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"h3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"had\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hadn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hadn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"happens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hardly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasnt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"have\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"haven\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"haven't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"having\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"help\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"her\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"here\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hereafter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hereby\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"herein\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"heres\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"here's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hereupon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"herself\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"him\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"himself\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"his\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hither\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ho\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"home\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hopefully\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"howbeit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"however\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hundred\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i6\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ibid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ie\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"if\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignored\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ih\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ij\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"il\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"im\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"immediate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"immediately\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"importance\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"important\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inasmuch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"indeed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"indicate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"indicated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"indicates\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"information\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"insofar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"instead\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"interest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"into\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"invention\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"io\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"isn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"isn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"itd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"its\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"itself\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"j\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"js\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ju\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"just\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ke\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keeps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kept\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"km\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"know\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"known\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"knows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ko\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"la\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"largely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lately\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"later\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latterly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"le\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"least\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"les\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"less\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"let\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"let's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"like\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"liked\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"likely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"line\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"little\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ln\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"look\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"looking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"looks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"los\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ltd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ma\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"made\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mainly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"make\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"makes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"many\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"may\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maybe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"me\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"means\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meantime\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meanwhile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"merely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"might\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mightn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mightn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mill\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"million\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"miss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"more\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"moreover\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"most\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mostly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"move\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ms\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"much\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mug\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"must\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mustn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mustn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"myself\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"na\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"namely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nay\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ne\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"near\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nearly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"necessarily\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"necessary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"need\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"needn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"needn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"needs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neither\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"never\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nevertheless\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"next\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ni\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ninety\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nobody\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"non\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nonetheless\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"noone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"normally\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"noted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nothing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"novel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"now\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nowhere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ns\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ny\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"obtain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"obtained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"obviously\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"od\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"off\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"often\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"og\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"okay\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"old\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"om\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"omitted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"once\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"one\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ones\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"only\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"onto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"op\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"or\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ord\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"os\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"other\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"others\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"otherwise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ou\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ought\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ours\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ourselves\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outside\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"over\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"overall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"owing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"own\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ox\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"page\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pagecount\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"par\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"part\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"particular\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"particularly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"past\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"per\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"perhaps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ph\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"placed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"please\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"plus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"po\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"poorly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"possible\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"possibly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"potentially\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predominantly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"present\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"presumably\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"previously\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"primarily\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probably\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"promptly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"proud\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"provides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"put\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"qj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"qu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"que\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quickly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"qv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ra\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ran\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rather\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"re\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readily\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"really\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reasonably\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recently\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ref\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"refs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"regarding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"regardless\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"regards\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"related\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relatively\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"research\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"research-articl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"respectively\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resulted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resulting\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ru\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ry\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"said\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"say\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saying\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"says\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"se\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"second\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"secondly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"section\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"see\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seeing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seemed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seeming\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seems\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"self\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"selves\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sensible\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serious\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seriously\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seven\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"several\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shan't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"should\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shouldn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shouldn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"should've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"show\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"showed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"showns\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"si\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"side\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"significant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"significantly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"similar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"similarly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"since\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sincere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"six\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sixty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"slightly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"so\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"some\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"somebody\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"somehow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"someone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"somethan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"something\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sometime\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sometimes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"somewhat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"somewhere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"soon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sorry\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"specifically\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"specified\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"specify\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"specifying\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"st\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"still\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strongly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"substantially\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"successfully\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"such\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sufficiently\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"suggest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"take\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"taken\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"taking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"td\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"te\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ten\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tends\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"th\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"than\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thank\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thanks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thanx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"the\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"their\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"theirs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"them\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"themselves\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"then\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thereafter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thereby\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thered\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"therefore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"therein\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thereof\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"therere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"theres\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thereto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thereupon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"these\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"theyd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"theyre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they're\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thickv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"think\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"third\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"this\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thorough\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thoroughly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"those\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thou\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"though\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thoughh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thousand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"three\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"throug\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"through\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"throughout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thru\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ti\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"til\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"together\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"too\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"towards\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tried\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tries\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"truly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"try\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trying\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"twelve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"twenty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"twice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"two\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"u\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"u201d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ui\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"um\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"un\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"under\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unfortunately\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unless\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unlike\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unlikely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"until\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"up\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"upon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ups\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ur\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"us\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"used\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"useful\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"usefully\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"usefulness\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uses\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"using\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"usually\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ut\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"va\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"various\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"very\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"via\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"viz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vols\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"volumtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"want\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wants\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"was\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wasn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wasnt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wasn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"way\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"welcome\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"well\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"well-b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"went\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"were\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we're\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weren\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"werent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weren't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whatever\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"when\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whenever\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"when's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"where\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whereafter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whereas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whereby\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wherein\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wheres\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"where's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whereupon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wherever\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whether\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"which\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"while\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whither\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whod\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whoever\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whole\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whom\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whomever\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"why\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"why's\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"widely\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"will\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"willing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wish\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"within\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"without\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"won\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wonder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wont\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"won't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"world\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"would\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wouldn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wouldnt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wouldn't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"www\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"youd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you'd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you'll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"your\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"youre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you're\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yours\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yourself\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yourselves\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you've\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ys\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zero\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mall_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstopwords_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstops\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtoken_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtoken_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_doc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mall_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'set' and 'list'"
     ]
    }
   ],
   "source": [
    "full_train_proc = preprocess(train_df, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting empty questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_len(question):\n",
    "    num_words = len(question.split())\n",
    "    return num_words\n",
    "\n",
    "#Dropping all the sample where there is not one of the two questions\n",
    "len_1   = full_train_proc.question1_cleaned.apply(lambda x: cont_len(x))\n",
    "empty_1 = len_1[len_1==0].index\n",
    "\n",
    "len_2   = full_train_proc.question2_cleaned.apply(lambda x: cont_len(x))\n",
    "empty_2 = len_2[len_2==0].index\n",
    "\n",
    "to_delete = (empty_2).union(set(empty_1))\n",
    "\n",
    "full_train_proc = full_train_proc.drop(index=to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CountVectorizer Solution\n",
    "### Naive CountVectorizer and attempts to improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T13:06:38.906060Z",
     "start_time": "2020-02-25T13:06:38.852578Z"
    }
   },
   "source": [
    "\n",
    "Use all the questions in train and test partitions to build a single list `all_questions` to fit the `count_vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:21:01.126503Z",
     "start_time": "2021-03-15T16:21:01.112286Z"
    }
   },
   "outputs": [],
   "source": [
    "all_questions = q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()\n",
    "df['question1_string'] = cast_list_as_strings(list(df[\"question1\"]))\n",
    "df['question2_string'] = cast_list_as_strings(list(df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T16:21:25.113927Z",
     "start_time": "2021-03-15T16:21:19.032577Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english', strip_accents='unicode')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer='word', binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 stop_words='english', #I also have a list of more advanced stopwords we could use\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 #token_pattern='(?u)\\b\\w\\w+\\b', #not sure why it doesn't work with the default option\n",
    "                                                                 preprocessor=None, tokenizer=None, vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function stemmed_words at 0x7f86a4e875e0>,\n",
       "                stop_words='english', strip_accents='unicode')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#advanced version with stemming\n",
    "#this implemenetation failed because stop_words cannot be used if analyzer!='word'\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer=stemmed_words, binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 stop_words='english', #I also have a list of more advanced stopwords we could use\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 #token_pattern='(?u)\\b\\w\\w+\\b', #not sure why it doesn't work with the default option\n",
    "                                                                 preprocessor=None, tokenizer=None, vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:504: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'tokenizer' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function stemmed_words at 0x7f86a4e87700>,\n",
       "                stop_words='english', strip_accents='unicode',\n",
       "                tokenizer=<__main__.LemmaTokenizer object at 0x7f8710b9d100>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#advanced version with lemmatization on top of stemming\n",
    "#this implemenetation failed because the tokenizer cannot be used if analyzer!='word'\n",
    "#additionally, if tokenizer!=None token_pattern and stop_words cannot be used\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer=stemmed_words, binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 stop_words='english', #I also have a list of more advanced stopwords we could use\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 #token_pattern='(?u)\\b\\w\\w+\\b', #not sure why it doesn't work with the default option\n",
    "                                                                 preprocessor=None, \n",
    "                                                                 tokenizer=LemmaTokenizer(), \n",
    "                                                                 vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with the improvement attempts so far\n",
    "\n",
    "* if analyzer=stemmed_words --> stop_words and tokenizer cannot be used\n",
    "* if tokenizer=LemmaTokenizer() --> token_pattern and stop_words cannot be used\n",
    "\n",
    "To solve these issues, we will not use the analyzer. We tokenize, remove the punctuation and stopwords, apply the regEx, and stem all inside the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final CountVectorizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list=nltk.corpus.stopwords.words('english')\n",
    "stops = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n",
    "stopwords_punctuation = (stopwords_list + list(set(stops) - set(stopwords_list)) + list(string.punctuation))\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = porter\n",
    "        self.lemmatizer = wordnet_lemmatizer\n",
    "    def __call__(self, articles):\n",
    "        tokens = re.compile(r'(?u)\\b\\w\\w+\\b').findall(articles) \n",
    "        tokens = [' '.join(w for w in token.split() if w.lower() not in stopwords_punctuation) for token in tokens]\n",
    "        tokens_stem = [self.stemmer.stem(t) for t in tokens]\n",
    "        tokens_lem = [self.lemmatizer.lemmatize(t) for t in tokens_stem]\n",
    "        return tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliocaggiano/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(strip_accents='unicode',\n",
       "                tokenizer=<__main__.Tokenizer object at 0x7fe9f87e2e80>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer=sklearn.feature_extraction.text.CountVectorizer(analyzer='word', binary=False, decode_error='strict', \n",
    "                                                                 encoding = 'utf-8', input='content', lowercase=True, \n",
    "                                                                 max_df=1.0, max_features=None, min_df=1,\n",
    "                                                                 ngram_range=(1,1), #let's play here with different combinations\n",
    "                                                                 strip_accents='unicode', #we can also try using ascii\n",
    "                                                                 preprocessor=None, \n",
    "                                                                 tokenizer=Tokenizer(), \n",
    "                                                                 vocabulary=None)\n",
    "\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Other Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding len Words in common feat\n",
    "def len_common(q1, q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1.intersection(q2))\n",
    "\n",
    "#Adding len common words in common feat\n",
    "def len_not_common(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    return len(q1 ^ q2)\n",
    "\n",
    "#Adding mean distance between common words \n",
    "def mean_dist_not_com(q1,q2):\n",
    "    q1 = set(word_tokenize(q1)) ; q2 = set(word_tokenize(q2))\n",
    "    not_comm1 = (q1 ^ q2) - q1\n",
    "    if len(not_comm1)==0 : not_comm1={''}\n",
    "    not_comm2 = (q1 ^ q2) - q2\n",
    "    if len(not_comm2)==0 : not_comm2={''}\n",
    "    return statistics.mean([editdistance.eval(i[0],i[1]) for i in itertools.product(not_comm1, not_comm2)])\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString)) \n",
    "def both_number(q1,q2):\n",
    "    return hasNumbers(q1) *  hasNumbers(q2) \n",
    "\n",
    "# get average number of words\n",
    "def average_len(question):\n",
    "    num_words = len(question.split())\n",
    "    return len(question)/num_words   \n",
    "\n",
    "# levenshtein distance (for strings of unequal length)\n",
    "def levenshtein(q1, q2): \n",
    "    #create initial array (two for loops since q1 and q2 can differ in length)\n",
    "    dist_array = []\n",
    "    for i in range(len(q1)+1):\n",
    "        dist_array.append([0]*(len(q2)+1))\n",
    "        dist_array[i][0] = i\n",
    "    for j in range(len(q2)+1):\n",
    "        dist_array[0][j] = j\n",
    "\n",
    "    dist = [0]*3\n",
    "    for i in range(1,len(q1)+1):\n",
    "        for j in range(1,len(q2)+1):\n",
    "            dist[0] = dist_array[i-1][j-1] if q1[i-1]==q2[j-1] else dist_array[i-1][j-1]+1\n",
    "            dist[1] = dist_array[i][j-1]+1\n",
    "            dist[2] = dist_array[i-1][j]+1\n",
    "            dist_array[i][j]=min(dist)\n",
    "    \n",
    "    return dist_array[i][j]\n",
    "\n",
    "# self implemented jaccard similarity\n",
    "#from math import*\n",
    " \n",
    "def jaccard_similarity(vector1,vector2):\n",
    "    jacc_num = 0 \n",
    "    jacc_den = 0 \n",
    "    for index in enumerate(vector1): \n",
    "        if vector1[index] != 0 or vector2[index] != 0: \n",
    "            jacc_den += max(vector1[index], vector2[index]) \n",
    "            jacc_num += min(vector1[index], vector2[index]) \n",
    "    return jacc_num / jacc_den\n",
    "\n",
    "def common_tokens(string_1,string_2):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    \n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    x = nltk.word_tokenize(string_1)\n",
    "    y = nltk.word_tokenize(string_2)   \n",
    " \n",
    "    common_tokens = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common_tokens)\n",
    "\n",
    "def common_count(string_1,string_2, word_type = \"NOUN\"):\n",
    "    \"\"\"\n",
    "    counts common word types. options are NOUN,VERB,ADV,ADJ\n",
    "    \"\"\"\n",
    "    string_1 = str(string_1)\n",
    "    string_2 = str(string_2)\n",
    "    \n",
    "    tagged_1 = nltk.pos_tag(nltk.word_tokenize(string_1),tagset='universal')\n",
    "    tagged_2 = nltk.pos_tag(nltk.word_tokenize(string_2),tagset='universal')\n",
    "    \n",
    "    x = list([])\n",
    "    for word in tagged_1:\n",
    "        if word[1] == word_type:\n",
    "            x.append(word[0])\n",
    "                     \n",
    "    y = list([])\n",
    "    for word in tagged_2:\n",
    "        if word[1] == word_type:\n",
    "            y.append(word[0])\n",
    " \n",
    "    common = len(list(set(x).intersection(y)))\n",
    "    \n",
    "    return(common)\n",
    "\n",
    "def length_diff_characters(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in character length of two questions\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    y = str(y)\n",
    "    return abs(len(y)-len(x))\n",
    "\n",
    "def length_diff_tokens(x,y):\n",
    "    \"\"\"\n",
    "    find absolute difference in number of tokens in two questions\n",
    "    \"\"\"\n",
    "    return abs(len(nltk.word_tokenize(y))-len(nltk.word_tokenize(x)))\n",
    "\n",
    "def common_numbers(x,y):\n",
    "    \"\"\"\n",
    "    count numbers present in both tokens\n",
    "    \"\"\"\n",
    "\n",
    "    x_numbers = re.findall(r'\\b\\d+\\b', x)\n",
    "    y_numbers = re.findall(r'\\b\\d+\\b', y)\n",
    "    \n",
    "    common_numbers = len(list(set(x_numbers).intersection(y_numbers)))\n",
    "    return(common_numbers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/pablogranatiero/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test[\"common_nouns\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"NOUN\"), axis=1)\n",
    "#df_test[\"common_verbs\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"VERB\"), axis=1)\n",
    "#df_test[\"common_adjectives\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"ADJ\"), axis=1)\n",
    "#df_test[\"common_adverbs\"] = df_test.apply(lambda df_test: common_count(df_test.question1, df_test.question2,\"ADV\"), axis=1)\n",
    "#df_test[\"common_tokens\"] = df_test.apply(lambda df_test: common_tokens(df_test.question1, df_test.question2), axis=1)\n",
    "#df_test[\"length_diff_characters\"] = df_test.apply(lambda df_test: length_diff_characters(df_test.question1, df_test.question2), axis=1)\n",
    "#df_test[\"length_diff_tokens\"] = df_test.apply(lambda df_test: length_diff_tokens(df_test.question1, df_test.question2), axis=1)\n",
    "#df_test[\"common_numbers\"] = df_test.apply(lambda df_test: length_diff_characters(df_test.question1, df_test.question2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_features(df, scaler, col1, col2, save=False):\n",
    "    df = df.copy()\n",
    "    col1_to_transform = list(full_train_proc.columns).index(col1)\n",
    "    col2_to_transform = list(full_train_proc.columns).index(col2)\n",
    "\n",
    "    df['len_q1'] = [len(s) for s in df[col1]] \n",
    "    df['len_q2'] = [len(s) for s in df[col2]]\n",
    "    df['len_q1'] = scaler.fit_transform(df[['len_q1']])\n",
    "    df['len_q2'] = scaler.fit_transform(df[['len_q2']])\n",
    "    \n",
    "    df['len_common'] = [len_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_common'] = scaler.fit_transform(df[['len_common']])\n",
    "\n",
    "    df['len_not_common'] = [len_not_common(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['len_not_common'] = scaler.fit_transform(df[['len_not_common']])\n",
    "    \n",
    "    df['mean_dist_not_com'] = [mean_dist_not_com(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['mean_dist_not_com'] = scaler.fit_transform(df[['mean_dist_not_com']])\n",
    "    \n",
    "    df['both_number'] = [both_number(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    \n",
    "    df[\"avg_len_q1\"]= df[col1].apply(lambda x: average_len(x))\n",
    "    df[\"avg_len_q2\"]= df[col2].apply(lambda x: average_len(x))\n",
    "    df['avg_len_q1'] = scaler.fit_transform(df[['avg_len_q1']])\n",
    "    df['avg_len_q2'] = scaler.fit_transform(df[['avg_len_q2']])\n",
    "\n",
    "    df['edit_distance'] = [editdistance.eval(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['edit_distance'] = scaler.fit_transform(df[['edit_distance']])\n",
    "    \n",
    "    df['levenshtein_distance'] = [levenshtein(df.iloc[i,col1_to_transform],df.iloc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    df['levenshtein_distance'] = scaler.fit_transform(df[['levenshtein_distance']])\n",
    "    \n",
    "    #df['manhattan_distance'] = [manhattan(df.loc[i,col1_to_transform],df.loc[i,col2_to_transform]) for i in range(len(df))]\n",
    "    #df['manhattan_distance'] = scaler.fit_transform(df[['manhattan_distance']])\n",
    "    df[\"common_nouns\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"NOUN\"), axis=1)\n",
    "    df['common_nouns'] = scaler.fit_transform(df[['common_nouns']])\n",
    "\n",
    "    df[\"common_verbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"VERB\"), axis=1)\n",
    "    df['common_verbs'] = scaler.fit_transform(df[['common_verbs']])\n",
    "    \n",
    "    df[\"common_adjectives\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADJ\"), axis=1)\n",
    "    df['common_adjectives'] = scaler.fit_transform(df[['common_adjectives']])\n",
    "    \n",
    "    df[\"common_adverbs\"] = df.apply(lambda df: common_count(df[col1], df[col2],\"ADV\"), axis=1)\n",
    "    df['common_adverbs'] = scaler.fit_transform(df[['common_adverbs']])\n",
    "\n",
    "    df[\"common_tokens\"] = df.apply(lambda df: common_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['common_tokens'] = scaler.fit_transform(df[['common_tokens']])\n",
    "\n",
    "    df[\"length_diff_characters\"] = df.apply(lambda df: length_diff_characters(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_characters'] = scaler.fit_transform(df[['length_diff_characters']])\n",
    "\n",
    "    df[\"length_diff_tokens\"] = df.apply(lambda df: length_diff_tokens(df[col1], df[col2]), axis=1)\n",
    "    df['length_diff_tokens'] = scaler.fit_transform(df[['length_diff_tokens']])\n",
    "\n",
    "    df[\"common_numbers\"] = df.apply(lambda df: length_diff_characters(df.question1, df.question2), axis=1)\n",
    "    df['common_numbers'] = scaler.fit_transform(df[['common_numbers']])\n",
    "\n",
    "    if save==True:\n",
    "        df.to_csv('features_added_Quora_full_train_data.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404269"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_proc_plus_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "full_train_proc_plus_feat = Add_features(full_train_proc, scaler, 'question1_cleaned', 'question2_cleaned', save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_proc_plus_feat.to_csv('features_added_Quora_full_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Two Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 2:  `get_features_from_df`\n",
    "\n",
    "Make a function `get_features_from_df` that given a dataframe containing the format of the train data\n",
    "it returns a scipy sparse matrix with the features from question 1 and question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_on_q1_q2(df, model):\n",
    "    q_list1 = list(df[\"question1_cleaned\"])\n",
    "    q_list2 = list(df[\"question2_cleaned\"])\n",
    "    all_questions = q_list1 + q_list2 \n",
    "    model.fit(all_questions)\n",
    "    return\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    ############### Begin exercise ###################\n",
    "    # what is kaggle                  q1\n",
    "    # What is the kaggle platform     q2\n",
    "    X_q1 = count_vectorizer.transform(q1_casted)\n",
    "    X_q2 = count_vectorizer.transform(q2_casted)    \n",
    "    X_q1q2 = scipy.sparse.hstack((X_q1,X_q2))\n",
    "    ############### End exercise ###################\n",
    "\n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_models(df, get_feat_model, vectorizer_func, col_list=0, add_feat=False):\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                       random_state=123)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, \n",
    "                              learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', eta=0.3, silent=1, subsample=0.8, random_state=123)\n",
    "\n",
    "    #svm_model = svm.SVC()\n",
    "    \n",
    "    train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.05,random_state=123)\n",
    "    train_df, val_df  = sklearn.model_selection.train_test_split(train_df, test_size=0.05,random_state=123)\n",
    "    X_train_q1q2      = get_feat_model(train_df, vectorizer_func)\n",
    "    X_val_q1q2        = get_feat_model(val_df, vectorizer_func)\n",
    "    X_test_q1q2       = get_feat_model(test_df, vectorizer_func)\n",
    "    \n",
    "    if add_feat==True:\n",
    "        X_train_q1q2      = add_a_column_feat_single(train_df, col_list, X_train_q1q2)\n",
    "        X_val_q1q2        = add_a_column_feat_single(val_df, col_list, X_val_q1q2)\n",
    "        X_test_q1q2       = add_a_column_feat_single(test_df, col_list, X_test_q1q2)\n",
    "    \n",
    "    y_train           = train_df[\"is_duplicate\"].values\n",
    "    y_val             = val_df[\"is_duplicate\"].values\n",
    "    y_test            = test_df[\"is_duplicate\"].values\n",
    "    \n",
    "    logistic.fit(X_train_q1q2, y_train)\n",
    "    xgb_model.fit(X_train_q1q2, y_train) \n",
    "    #svm_model.fit(X_tr_q1q2, y_train)\n",
    "                                                   \n",
    "    logistic_train_acc = roc_auc_score(y_train, logistic.predict_proba(X_train_q1q2)[:, 1])                                                       \n",
    "    logistic_val_acc   = roc_auc_score(y_train, logistic.predict_proba(X_val_q1q2)[:, 1])\n",
    "    logistic_test_acc  = roc_auc_score(y_train, logistic.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('logistic_train_acc:{}, logistic_val_acc:{}, logistic_test_acc:{}'.format(logistic_train_acc, logistic_val_acc, logistic_test_acc))\n",
    "                                                   \n",
    "    xgb_train_acc      = roc_auc_score(y_train, xgb_model.predict_proba(X_train_q1q2)[:, 1])\n",
    "    xgb_val_acc        = roc_auc_score(y_val, xgb_model.predict_proba(X_val_q1q2)[:, 1])\n",
    "    xgb_test_acc       = roc_auc_score(y_test, xgb_model.predict_proba(X_test_q1q2)[:, 1])\n",
    "    print('xgb_train_acc:{}, xgb_val_acc:{}, xgb_test_acc:{}'.format(xgb_train_acc, xgb_val_acc, xgb_test_acc))\n",
    "                                                   \n",
    "    return logistic, xgb_model, [logistic_train_acc, logistic_val_acc, logistic_test_acc], [xgb_train_acc, xgb_val_acc, xgb_test_acc]\n",
    "                                              \n",
    "                                                       \n",
    "                                                       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8560015673032263, logistic_val_acc:0.7953934745032775, logistic_test_acc:0.7956985843493918\n",
      "xgb_train_acc:0.8659191549450259, xgb_val_acc:0.8093737107889942, xgb_test_acc:0.8097783288425393\n"
     ]
    }
   ],
   "source": [
    "logistic_simple, xgb_model_simple, acc_logistic_simple, acc_xgboost_simple = train_models(full_train_proc_plus_feat, get_features_from_df, count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic and a XGB on the cleaned questions using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=False):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question1_cleaned\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "fit_on_q1_q2(full_train_proc_plus_feat, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:59:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.8416865189699401, logistic_val_acc:0.7958238445455491, logistic_test_acc:0.797239576925177\n",
      "xgb_train_acc:0.9070173792036841, xgb_val_acc:0.8234103363606295, xgb_test_acc:0.827242512987826\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf, xgb_model_simple_tf, acc_logistic_simple_tf, acc_xgboost_simple_tf = train_models(full_train_proc_plus_feat, get_tfidf, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. More complex models - Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(df, tfidf, sim=True):\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))\n",
    "    \n",
    "    tfidf_q1 = tfidf.transform(q1_casted)\n",
    "    tfidf_q2 = tfidf.transform(q2_casted)\n",
    "    tfidf_q1q2 = scipy.sparse.hstack((tfidf_q1,tfidf_q2))\n",
    "    if sim == True:\n",
    "        sims = []\n",
    "        for i in range(len(q1_casted)):\n",
    "            sims.append(cosine_similarity(tfidf_q1[i,:],tfidf_q2[i,:]))\n",
    "        sims = np.reshape(sims, (len(q1_casted), 1))\n",
    "\n",
    "        return scipy.sparse.hstack((tfidf_q1q2,sims)).tocsr() \n",
    "    else:\n",
    "        return tfidf_q1q2.tocsr() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablogranatiero/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:56:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logistic_train_acc:0.9074969597126898, logistic_val_acc:0.8726778599886169, logistic_test_acc:0.8772466341269656\n",
      "xgb_train_acc:0.9766716766767485, xgb_val_acc:0.8987541590750521, xgb_test_acc:0.9019259149390173\n"
     ]
    }
   ],
   "source": [
    "logistic_simple_tf_sim, xgb_model_simple_tf_sim, acc_logistic_tf_sim, acc_xgboost_tf_sim = train_models(full_train_proc_plus_feat, get_tfidf, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features based on lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_a_column_feat(df, col_list, sparse_matrix):\n",
    "\n",
    "    for col_q in col_list:\n",
    "        feat_q = df[col_q].to_numpy().reshape(len(df[col_q]),1)\n",
    "        sparse_matrix = scipy.sparse.hstack((sparse_matrix,feat_q)).tocsr()\n",
    "    \n",
    "    return sparse_matrix     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_lenght = ['len_q1', 'len_q2', 'len_common', 'len_not_common']\n",
    "logistic_simple_tf_sim_len, xgb_model_simple_tf_sim_len, acc_logistic_tfidf_sim_len, acc_xgboost_tfidf_sim_len = train_models(full_train_proc_plus_feat, get_tfidf, tfidf, [cols_lenght], add_feat=True)     \n",
    "                                                                                                                              \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
